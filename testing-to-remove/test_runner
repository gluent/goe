#! /usr/bin/env python3
""" Parent control script for running integration tests.
    We are migrating sets of tests across from 'test' to 'test_set_runner'.
    LICENSE_TEXT
"""

from optparse import OptionGroup

from gluent import get_common_options, get_log_fh, init, init_log
from gluentlib.config.orchestration_config import OrchestrationConfig
from gluentlib.offload.offload_messages import OffloadMessages

from testlib.test_framework import test_constants
from testlib.test_framework.test_functions import add_common_test_options, add_common_test_run_options,\
    add_story_test_options, add_test_set_options, normalise_test_pass_options
from testlib.test_framework.test_results import END_OF_TEST_RUN_MARKER
from test_sets.benchmarks import Benchmarks
from test_sets.stories.story_runner import list_stories
from test_sets.test_set_runner import TestSetRunner


def get_test_options():
    opt = get_common_options()
    add_common_test_options(opt)
    add_test_set_options(opt, [test_constants.SET_SQL_OFFLOAD, test_constants.SET_TYPE_MAPPING])
    add_common_test_run_options(opt)

    story_options = OptionGroup(opt, "Story Options", "Options relating to orchestration stories")
    add_story_test_options(opt)
    opt.add_option_group(story_options)

    # Strip away the options we don't need
    for option in ['--skip-steps']:
        opt.remove_option(option)

    return opt


if __name__ == '__main__':

    opt = get_test_options()
    options, _ = opt.parse_args()
    init(options)
    init_log('test')

    normalise_test_pass_options(options)

    config = OrchestrationConfig.from_dict({
            'execute': options.execute,
            'verbose': options.verbose,
            'vverbose': options.vverbose,
        })
    messages = OffloadMessages.from_options(options, get_log_fh())

    # TODO Need to migrate blacklist code (perhaps organised by frontend/backend combo?)

    test_runner = TestSetRunner(options.test_user, options.test_pass, options.test_hybrid_pass, messages, config,
                                options.filter, dry_run=bool(not options.execute),
                                teamcity=options.teamcity, known_failure_blacklist=[], ansi=options.ansi,
                                run_blacklist_only=options.run_blacklist_only, test_pq_degree=options.test_pq_degree,
                                story=options.story)

    if options.list_stories:
        list_stories(options, messages, config)
    else:
        benchmarks = Benchmarks()
        benchmarks.load()

        if options.test_set is None:
            options.test_set = test_constants.ALL_TEST_SETS
        else:
            options.test_set = options.test_set.split(',')

        # Running each named set in the order they were requested
        for test_set in options.test_set:
            test_runner.run_set(test_set)

        benchmarks.save()

        messages.log(END_OF_TEST_RUN_MARKER)
