# ===========================================================================================
# Google BigQuery settings
# ===========================================================================================

# Path to Google service account private key JSON file
# export GOOGLE_APPLICATION_CREDENTIALS=

# Backend distribution:
export BACKEND_DISTRIBUTION=GCP

# Orchestration query engine
export QUERY_ENGINE=BIGQUERY

# Google BigQuery location to use when creating a dataset, this has no impact other than when creating datasets.
# The default is to use the BigQuery default.
# Note the dataset location must be compatible with that of the bucket specified in OFFLOAD_FS_CONTAINER
export BIGQUERY_DATASET_LOCATION=

# Project to use for BigQuery table references.
# The default is to use the default project for the authenticated user/service account.
export BIGQUERY_DATASET_PROJECT=

# Google Cloud Key Management Service crytopgraphic key information for customer-managed encryption keys (CMEK)
# GOOGLE_KMS_KEY_RING_PROJECT only needs to be set if the KMS project differs from the default
# project for the authenticated user/service account.
export GOOGLE_KMS_KEY_RING_PROJECT=
export GOOGLE_KMS_KEY_RING_LOCATION=
export GOOGLE_KMS_KEY_RING_NAME=
export GOOGLE_KMS_KEY_NAME=

# Google Dataproc cluster name
export GOOGLE_DATAPROC_CLUSTER=
# Google Dataproc/Dataproc Serverless project
export GOOGLE_DATAPROC_PROJECT=
# Google Dataproc/Dataproc Serverless region
export GOOGLE_DATAPROC_REGION=
# Google Dataproc/Dataproc Serverless service account
export GOOGLE_DATAPROC_SERVICE_ACCOUNT=
# Google Dataproc Batches version, leave blank to disable Dataproc Batches
export GOOGLE_DATAPROC_BATCHES_VERSION=
# Google Dataproc Batches subnet name
# If set then this variable is used to form a value for Batches subnet of this form:
# projects/${GOOGLE_DATAPROC_PROJECT}/regions/${GOOGLE_DATAPROC_REGION}/subnetworks/${GOOGLE_DATAPROC_BATCHES_SUBNET}
# export GOOGLE_DATAPROC_BATCHES_SUBNET=

# Filesystem type for Offloaded tables
# When offloading a table to cloud storage the table LOCATION will be structured as below:
#   ${OFFLOAD_FS_SCHEME}://${OFFLOAD_FS_CONTAINER}/${OFFLOAD_FS_PREFIX}/db_name/table_name/
export OFFLOAD_FS_SCHEME=gs
# The path with which to prefix offloaded table paths.
export OFFLOAD_FS_PREFIX=goe
# A valid bucket or container name when offloading to cloud storage
export OFFLOAD_FS_CONTAINER=

# File format for staged data during an Offload (supported values: AVRO and PARQUET)
export OFFLOAD_STAGING_FORMAT=AVRO

# Key/value pairs, in JSON format, defining session query parameters for the orchestration backend query engine.
# These take effect for all queries issued to the query engine, e.g:
#     export OFFLOAD_BACKEND_SESSION_PARAMETERS="{\"parameter_name\": \"some.value\"}"
#export OFFLOAD_BACKEND_SESSION_PARAMETERS=

# Case conversion to be applied to any backend identifier names created by GOE (supported values: UPPER, LOWER and NO_MODIFY).
export BACKEND_IDENTIFIER_CASE=LOWER

# Authentication mechanism for Spark ThriftServer
export HIVE_SERVER_AUTH_MECHANISM=PLAIN
