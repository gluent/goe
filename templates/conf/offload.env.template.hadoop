# Copyright 2016 The GOE Authors. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# ===========================================================================================
# Hadoop settings
# ===========================================================================================

# Hadoop/FS settings:
HADOOP_SSH_USER=${OFFLOAD_TRANSPORT_USER}
HDFS_HOME=/user/goe
# The HDFS location for both temporary load and final offloaded tables. OFFLOAD_FS_PREFIX, when set, takes
# precedence over HDFS_DATA for Offloaded tables
HDFS_DATA=${HDFS_HOME}/offload
HDFS_LOAD=${HDFS_DATA}

# Filesystem type for Offloaded tables. Valid values are:
#   inherit: Do not include a LOCATION clause when creating a table, inherit the value from the container database
#   hdfs:    Use a LOCATION clause to store table data in HDFS in the local cluster
#   s3a:     Use a LOCATION clause to store table data in Amazon S3. This must be correctly configured in the backend system configuration
#   adl:     Use a LOCATION clause to store table data in Microsoft Azure Data Lake Storage Generation 1. This must be correctly configured in the backend system configuration
#   abfs(s): Use a LOCATION clause to store table data in Microsoft Azure Data Lake Storage Generation 2. This must be correctly configured in the backend system configuration
# When offloading a table to cloud storage the table LOCATION will be structured as below:
#   ${OFFLOAD_FS_SCHEME}://${OFFLOAD_FS_CONTAINER}/${OFFLOAD_FS_PREFIX}/db_name/table_name/
OFFLOAD_FS_SCHEME=inherit
# The path with which to prefix offloaded table paths. Takes precedence over HDFS_DATA when OFFLOAD_FS_SCHEME != "inherit"
OFFLOAD_FS_PREFIX=${HDFS_DATA}
# A valid bucket or container name when offloading to cloud storage
OFFLOAD_FS_CONTAINER=

# Distribute data by partition key(s) during the final INSERT operation of an offload. Hive only (will be ignored for Impala).
# Defaults to true
OFFLOAD_DISTRIBUTE_ENABLED=true

# HDFS client configuration file location
#LIBHDFS3_CONF=${OFFLOAD_HOME}/conf/hdfs-client.xml

# WEBHDFS_HOST/PORT can be used to optimize HDFS activities removing JVM start-up overhead by utilising WebHDFS
# WEBHDFS_HOST can be a comma-separated list of hosts if HDFS is configured for High Availability
# If WEBHDFS_PORT is unset then default ports of 50070 (HTTP) or 50470 (HTTPS) are used
# WEBHDFS_VERIFY_SSL is used to enable SSL for WebHDFS calls. There are 4 states:
#   Empty: Do not use SSL
#   TRUE: Use SSL & verify Hadoop certificate against known certificates
#   FALSE: Use SSL & do not verify Hadoop certificate
#   /some/path/here/cert-bundle.crt: Use SSL & verify Hadoop certificate against path to certificate bundle
#WEBHDFS_HOST=
#WEBHDFS_PORT=
WEBHDFS_VERIFY_SSL=

# Impala/Hive connection settings:
# (HIVE_SERVER_HOST/PORT are used for both Impala and Hive connections)
# HIVE_SERVER_HOST can be a comma-separated list of hosts to randomly choose from, e.g.: hdp21,hdp22,hdp23
# Default Impala port is 21050, default Hive port is 10000
HIVE_SERVER_HOST=<hostname>
HIVE_SERVER_PORT=21050
HIVE_SERVER_USER=goe
HIVE_SERVER_PASS=
# Use HTTP transport for HiveServer2 connections
#HIVE_SERVER_HTTP_TRANSPORT=true
# Path component of URL endpoint when connecting to HiveServer2 in HTTP mode
#HIVE_SERVER_HTTP_PATH=
# Path to LDAP password file
#HIVE_SERVER_LDAP_PASSWORD_FILE=
# Hive connection timeout in seconds
HIVE_SERVER_TIMEOUT=3600

# Authentication mechanism for HiveServer2
# In non-kerberized environments, should be set to:
#   impala: NOSASL
#   hive:   value of hive-site.xml: hive.server2.authentication
# Ignored in kerberized or LDAP environments
#HIVE_SERVER_AUTH_MECHANISM=NOSASL

# Kerberos settings:
# KERBEROS_SERVICE is the SQL engine Kerberos service (usually 'impala' or 'hive')
# KERBEROS_KEYTAB is the path of the keytab file
#  if not provided, a valid ticket must already exist in the cache (i.e. manual kinit)
# if KERBEROS_KEYTAB is provided, KERBEROS_PRINCIPAL should also be provided
#  it is the kerberos user to authenticate as. ie $kinit -kt KERBEROS_KEYTAB KERBEROS_PRINCIPAL should succeed
# Set KERBEROS_PATH if your Kerberos utilities (like kinit) reside in some non-standard directory
KERBEROS_SERVICE=
KERBEROS_KEYTAB=
KERBEROS_PRINCIPAL=
KERBEROS_PATH=/usr/kerberos/bin
# KERBEROS_TICKET_CACHE_PATH is required to use the libhdfs3-based HDFS result cache in a kerberized cluster
# For example: /tmp/krb5cc_12345
KERBEROS_TICKET_CACHE_PATH=

# Paths
PATH=${PATH}:${KERBEROS_PATH}

# ===========================================================================================
# Advanced settings: you probably do not need to modify the lines below
# ===========================================================================================
# HDFS_CMD_HOST overrides HIVE_SERVER_HOST for HDFS operations only
HDFS_CMD_HOST=

# Databases are named <schema><HDFS_DB_PATH_SUFFIX> and <schema>_load<HDFS_DB_PATH_SUFFIX>
#  HDFS_DB_PATH_SUFFIX defaults to .db, giving <schema>.db and <schema>_load.db
#  Uncomment the following line if the .db is causing problems
#HDFS_DB_PATH_SUFFIX=

# Backend distribution 'override' (supported values: CDH, HDP, EMR, MAPR)
BACKEND_DISTRIBUTION=CDH

# Case conversion to be applied to any backend identifier names created by GOE (supported values: UPPER, LOWER and NO_MODIFY).
BACKEND_IDENTIFIER_CASE=LOWER

# Key/value pairs, in JSON format, defining session query parameters for the orchestration backend query engine.
# These take effect for all queries issued to the query engine, e.g:
#     OFFLOAD_BACKEND_SESSION_PARAMETERS="{\"request_pool\": \"goe.pool\"}"
#OFFLOAD_BACKEND_SESSION_PARAMETERS=

# Orchestration query engine (supported values: IMPALA, HIVE)
QUERY_ENGINE=IMPALA

# Comma-delimited list of HiveServer2 session parameters to set
# BATCH_SIZE=16384 is a strongly recommended performance setting
# eg. HS2_SESSION_PARAMS="BATCH_SIZE=16384,MEM_LIMIT=2G"
HS2_SESSION_PARAMS="BATCH_SIZE=16384"

# An alias provided by Hadoop "credential provided API" to be used for RDBMS authentication
# OFFLOAD_TRANSPORT_PASSWORD_ALIAS=
# # The credential provider path to be used in conjunction with OFFLOAD_TRANSPORT_PASSWORD_ALIAS, e.g.:
# #     "jceks://hdfs/user/goe/dbname.dbuser.pwd.m.jceks"
# # Only required if the path is not configured in Hadoop configuration defaults
# OFFLOAD_TRANSPORT_CREDENTIAL_PROVIDER_PATH=

# Sqoop settings:
# You should be using OraOOP optimizations for Sqoop (included in standard Apache Sqoop
# from v1.4.5), but if you're not, then you need to disable direct path mode:
SQOOP_DISABLE_DIRECT=false
# Override flags for sqoop command, inserted right after "sqoop import", e.g.:
#     "-Dhadoop.security.credential.provider.path=jceks://hdfs/user/goe/dbname.dbuser.pwd.m.jceks"
# This setting is ignored for OFFLOAD_TRANSPORT_METHODs that do not utilise sqoop
SQOOP_OVERRIDES="-Dsqoop.avro.logical_types.decimal.enable=false"
# Add sqoop command options at the end of the sqoop command
SQOOP_ADDITIONAL_OPTIONS=
# HDFS path to Sqoop password file, readable by HADOOP_SSH_USER. If not specified, ORA_APP_PASS or OFFLOAD_TRANSPORT_PASSWORD_ALIAS will be used
SQOOP_PASSWORD_FILE=
# Yarn queue name for GOE Sqoop jobs
SQOOP_QUEUE_NAME=
