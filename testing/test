#! /usr/bin/env python3
# Copyright 2015 Gluent Inc. All rights reserved

import faulthandler; faulthandler.enable()
import copy
from datetime import datetime, date, timedelta, time as dt_time
import decimal
from decimal import Decimal
from distutils.version import LooseVersion
import glob
import inspect
import json
import logging
import math
import multiprocessing
from optparse import SUPPRESS_HELP
import os.path
import pprint
import random
from random import choice
import re
import subprocess
import textwrap
import threading
import time
import traceback
import uuid
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
from threading import BoundedSemaphore

import cx_Oracle as cxo
import dateutil.parser

from gluent import get_db_charset, get_log_fh, get_options, \
    get_oracle_options, init, init_log, normalise_options, verbose, vverbose
from offload_status_report import normalise_offload_status_report_options, OffloadStatusReport, \
    DEFAULT_REPORT_NAME, DEFAULT_REPORT_DIRECTORY, DEFAULT_CSV_DELIMITER, DEFAULT_CSV_ENCLOSURE,\
    CSV, JSON, RAW, HTML, TEXT, DETAIL_LEVEL, SUMMARY_LEVEL
from diagnose import DEFAULT_IMPALAD_HTTP_PORT, DEFAULT_HS2_WEBUI_PORT, DEFAULT_LLAP_WEBUI_PORT, \
    DEFAULT_SPARK_HISTORY_PORT, SPARK_HISTORIC_APPLICATION_DAYS
from gluentlib.config import orchestration_defaults
from gluentlib.config.orchestration_config import OrchestrationConfig
from gluentlib.config.config_validation_functions import normalise_rdbms_options
from gluentlib.offload.column_metadata import GLUENT_TYPE_DATE, GLUENT_TYPE_DECIMAL, \
    GLUENT_TYPE_DOUBLE, GLUENT_TYPE_FLOAT,\
    GLUENT_TYPE_INTEGER_1, GLUENT_TYPE_INTEGER_2, GLUENT_TYPE_INTEGER_4, GLUENT_TYPE_INTEGER_8, \
    GLUENT_TYPE_INTEGER_38, GLUENT_TYPE_TIME, GLUENT_TYPE_TIMESTAMP, GLUENT_TYPE_TIMESTAMP_TZ,\
    GLUENT_TYPE_VARIABLE_STRING, GLUENT_TYPE_FIXED_STRING, GLUENT_TYPE_LARGE_STRING, \
    GLUENT_TYPE_BINARY, GLUENT_TYPE_LARGE_BINARY
from gluentlib.offload.frontend_api import extract_connection_details_from_dsn
from gluentlib.offload.microsoft.mssql_frontend_api import setup_mssql_session
from gluentlib.offload.netezza.netezza_frontend_api import setup_netezza_session
from gluentlib.offload.offload_constants import DBTYPE_BIGQUERY, DBTYPE_HIVE, DBTYPE_IMPALA, \
    DBTYPE_ORACLE, DBTYPE_SNOWFLAKE, DBTYPE_SPARK, DBTYPE_SYNAPSE,\
    HADOOP_BASED_BACKEND_DISTRIBUTIONS
from gluentlib.offload.offload_messages import OffloadMessages, VERBOSE, VVERBOSE
from gluentlib.offload.oracle.oracle_column import ORACLE_TYPE_CHAR, ORACLE_TYPE_NCHAR, ORACLE_TYPE_CLOB,\
    ORACLE_TYPE_NCLOB, ORACLE_TYPE_VARCHAR, ORACLE_TYPE_VARCHAR2, ORACLE_TYPE_NVARCHAR2, ORACLE_TYPE_RAW,\
    ORACLE_TYPE_BLOB, ORACLE_TYPE_NUMBER, ORACLE_TYPE_FLOAT, ORACLE_TYPE_BINARY_FLOAT, ORACLE_TYPE_BINARY_DOUBLE,\
    ORACLE_TYPE_DATE, ORACLE_TYPE_TIMESTAMP, ORACLE_TYPE_TIMESTAMP_TZ, ORACLE_TIMESTAMP_TZ_SUB_PATTERN,\
    ORACLE_TIMESTAMP_TZ_RE, ORACLE_INTERVAL_DS_RE, ORACLE_INTERVAL_YM_RE, ORACLE_TYPE_INTERVAL_DS,\
    ORACLE_INTERVAL_DS_SUB_PATTERN, ORACLE_TYPE_INTERVAL_YM, ORACLE_INTERVAL_YM_SUB_PATTERN
from gluentlib.orchestration import command_steps, orchestration_constants
from gluentlib.orchestration.command_steps import step_title
from gluentlib.orchestration.orchestration_runner import OrchestrationRunner
from gluentlib.persistence.factory.orchestration_repo_client_factory import orchestration_repo_client_factory

from testlib.setup import gen_test_data, setup_constants
from testlib.setup.test_setup_client import create_partition_functions
from testlib.test_framework import test_constants
from testlib.test_framework.backend_testing_api import TRANSIENT_QUERY_RERUN_MARKER
from testlib.test_framework.test_functions import add_common_test_options, add_common_test_run_options,\
    add_story_test_options, add_test_set_options,\
    get_backend_db_table_name_from_metadata, \
    get_backend_testing_api, get_frontend_testing_api, get_lines_from_log, get_data_db_for_schema,\
    get_naughty_state, get_test_set_sql_path, gl_wide_max_columns, log,\
    normalise_test_pass_options, response_time_bench, test_passes_filter, to_hybrid_schema,\
    teamcity_escape, test_teamcity_starttestsuite, test_teamcity_starttestsuite_pq, test_teamcity_endtestsuite,\
    test_teamcity_endtestsuite_pq, test_teamcity_endtest, test_teamcity_endtest_pq,\
    test_teamcity_failtest, test_teamcity_failtest_pq,\
    test_teamcity_starttest, test_teamcity_starttest_pq
from testlib.test_framework.test_results import TestResults, normalize_state, \
    END_OF_TEST_RUN_MARKER, STATE_FAIL, STATE_ERROR
from testlib.test_framework.teamcity_integration import *
from testlib.test_framework.test_value_generators import TestDecimal

from gluentlib.util.diagnostics import Diagnose, PermissionsTuple
from gluentlib.util.linux_cmd import LinuxCmd
from gluentlib.util.misc_functions import load_yaml_in_order

from selenium import webdriver
from selenium.webdriver.firefox.service import Service
from selenium.webdriver.firefox.options import Options
from selenium.webdriver.common.by import By
from selenium.common.exceptions import NoSuchElementException

from tests.offload.test_backend_table import partition_key_test_numbers

from test_sets.base_test import BaseTest, TestFailure, reset_tty
from test_sets.benchmarks import Benchmarks
from test_sets.known_test_failures import gen_known_failure_blacklist, GOE_1546_KNOWN_FAILURE_BLACKLIST,\
    GOE_1764_KNOWN_FAILURE_BLACKLIST, SINGLE_BYTE_CHARSET_KNOWN_FAILURE_BLACKLIST
from test_sets.offload_test_functions import get_suitable_offload_granularity_options, \
    get_suitable_offload_partition_name
from test_sets.offload_transport.offload_transport_tests import run_offload_transport_tests
from test_sets.python_unit.unit_test_runner import run_python_unit_tests
from test_sets.query_hint_builder import QueryHintBuilder
from test_sets.sql_offload.sql_offload_tests import run_sql_offload_tests, sql_offload_test_table_scripts
from test_sets.stories.story_globals import STORY_TYPE_AGG_VALIDATE, \
    STORY_TYPE_LINK_ID, STORY_TYPE_OFFLOAD, STORY_TYPE_PRESENT, \
    STORY_TYPE_SETUP, STORY_TYPE_SCHEMA_SYNC, STORY_TYPE_SHOW_BACKEND_STATS,\
    STORY_TYPE_SHELL_COMMAND, STORY_TYPE_PYTHON_FN
from test_sets.stories.story_assertion_functions import clob_safe_columns, desc_columns
from test_sets.stories import story_runner
from test_sets.stories.story_setup_functions import gen_max_length_identifier
from test_sets.type_mapping.type_mapping_tests import run_type_mapping_offload_column_tests,\
    run_type_mapping_offload_test


class GluentTestException(Exception):
    pass


known_failure_blacklist = []

GL_TYPE_MAPPING_TABLE = 'gl_type_mapping'

GL_IDENTIFIERS_TABLE = 'gl_identifiers'

# In re below (?: ) is a non-capturing group preventing me from picking up the required (but also unwanted) underscore
SQL_CUSTOM_FILE_NAME_RE = re.compile(r'.*\/(custom_q[0-9]+)(?:_([a-z]+))?.sql$')

PART_LIST_TABLE_DATE = 'gl_part_list_date'
PART_LIST_TABLE_NUM = 'gl_part_list_num'
PART_LIST_AS_RANGE_TABLE_DATE = 'gl_part_list_range_date'
PART_LIST_AS_RANGE_TABLE_NUM = 'gl_part_list_range_num'
PART_RANGE_TABLE_DATE = 'gl_part_range_date'
PART_RANGE_TABLE_DATE_DATE = 'gl_part_range_date_date'
PART_RANGE_TABLE_NUM = 'gl_part_range_num'
PART_RANGE_TABLE_DEC = 'gl_part_range_dec'
PART_RANGE_TABLE_DEC2 = 'gl_part_range_dec2'
PART_RANGE_NUM_VALUES = [-4, -3, -2, -1, 0, 1, 2, 3, 4]
PART_RANGE_TABLE_STR = 'gl_part_range_str'
# Minus plays havoc with string sorting so just use positive test values
PART_RANGE_STR_VALUES = [str(_) for _ in PART_RANGE_NUM_VALUES if _ >= 0]
PART_RANGE_STR_HVS = [str(_+1) for _ in PART_RANGE_NUM_VALUES if _ >= 0]

# These values are pure dates because they may be offloaded to a canonical date
PART_RANGE_DATE_VALUES = [datetime(2020, 8, 1), datetime(2020, 9, 1), datetime(2020, 10, 1),
                          datetime(2020, 11, 1), datetime(2020, 12, 1)]
COUNT_STAR = 'COUNT(*)'
GL_PART_COL_RE = re.compile(r'GL_PART_[0-9DMYU]+_COLUMN_1', flags=re.I)

# Sets precision for decimal conversions to 40 digits, two more than Oracle.
decimal.getcontext().prec = 40

dense = pprint.PrettyPrinter(indent=2, width=240)

dev_logger = logging.getLogger("test")

default_parallel = 2

response_time_fail_increase_pc = 10


class LoggingOraCursor(object):
    """ ORACLE cursor proxy, but with statement/results logging
    """
    def __init__(self, cursor):
        self._cursor = cursor

    def _execute(self, sql, *args, **kwargs):
        dev_logger.info("\n[ORACLE SQL]\n%s\n[END ORACLE SQL]" % sql)
        self._cursor.execute(sql, *args, **kwargs)
        dev_logger.debug("\n[BINDS]\n%s\n[END BINDS]" % self._cursor.bindvars)

        return self

    def _executemany(self, sql, *args, **kwargs):
        dev_logger.info("\n[ORACLE BULK SQL]\n%s\n[END ORACLE BULK SQL]" % sql)
        self._cursor.execute(sql, *args, **kwargs)
        dev_logger.debug("\n[BINDS]\n%s\n[END BINDS]" % self._cursor.bindvars)

        return self

    def _fetchone(self):
        results = self._cursor.fetchone()
        dev_logger.debug("\n[ORACLE FETCHONE]\n%s\n[END ORACLE FETCHONE]" % results)
        return results

    def _fetchall(self):
        results = self._cursor.fetchall()
        dev_logger.debug("\n[ORACLE FETCHALL]\n%s\n[END ORACLE FETCHALL]" % results)
        return results

    def __getattr__(self, name):
        if name == 'execute':
            return self._execute
        elif name == 'executemany':
            return self._executemany
        elif name == 'fetchone':
            return self._fetchone
        elif name == 'fetchall':
            return self._fetchall
        else:
            return getattr(self._cursor, name)


def gen_test_id():
    return uuid.uuid4().hex


def get_log_level():
    levels = {'info' : 1, 'detail' : 2, 'debug': 3}
    ll = os.environ.get('LOG_LEVEL').lower()
    if not ll:
        ll = 'info'
    return [ll, levels[ll]]


def get_test_cursor(options, threaded=True, hybrid=False, admin=False, gluent_adm=False):
    if not options:
        return None

    if options.source_db_type == 'oracle':
        if hybrid:
            cursor = cxo.connect(get_hybrid_schema(options.test_user), options.test_hybrid_pass,
                                 options.oracle_dsn,
                                 threaded=threaded).cursor()
            cursor.connection.action = 'get_test_cursor(hybrid)'
        elif admin:
            if options.ora_test_adm_user == options.ora_adm_user:
                # This password may have been decrypted so let's make sure we use the decrypted version.
                options.ora_test_adm_pass = options.ora_adm_pass
            cursor = cxo.connect(options.ora_test_adm_user, options.ora_test_adm_pass,
                                 options.oracle_dsn,
                                 threaded=threaded).cursor()
            cursor.connection.action = 'get_test_cursor(admin)'
        elif gluent_adm:
            cursor = cxo.connect(options.ora_adm_user, options.ora_adm_pass, options.oracle_dsn,
                                 threaded=threaded).cursor()
            cursor.connection.action = 'get_test_cursor(gluent_adm)'
        else:
            cursor = cxo.connect(options.test_user, options.test_pass, options.oracle_dsn,
                                 threaded=threaded).cursor()
            cursor.connection.action = 'get_test_cursor'
        cursor.connection.module = 'Test'
    elif options.source_db_type == 'mssql':
        server, port, database = extract_connection_details_from_dsn(options.mssql_dsn)
        db_conn = setup_mssql_session(server, port, database, options.test_user, options.test_pass, 'Gluent test')
        cursor = db_conn.cursor()
    elif options.source_db_type == 'netezza':
        server, port, database = extract_connection_details_from_dsn(options.netezza_dsn)
        db_conn = setup_netezza_session(server, port, database, options.test_user, options.test_pass)
        cursor = db_conn.cursor()

    if options.dev_log_level.upper() in ('INFO', 'DEBUG') and options.source_db_type == 'oracle':
        return LoggingOraCursor(cursor)
    else:
        return cursor


global test_lines
test_lines = 0

# TestResults analysis instance
global g_test_results
g_test_results = None


class Test(BaseTest):
    def __init__(self, options, name, test_callable):
        super().__init__(options, name, test_callable)
        self.offload_messages = None
        self.query_key = ''

    def __call__(self, quiet=False, cursor=None, adm_cursor=None, close_cursor=False, cursor_factory=None,
                 parent_flow_id=None, capture_stdout=False):
        """ Override BaseTest.__call__ in order to add Oracle specific cursor management. """
        result, result_message = None, ""
        fn_result = None
        # test the cursor connection
        if cursor:
            try:
                # test connection
                cursor.connection.ping()
            except:
                # connection dead
                self.cursor = get_test_cursor(self._options)
                close_cursor = True
            else:
                # connection alive
                self.cursor = cursor
        elif cursor_factory:
            self.cursor = cursor_factory()
            close_cursor = True
        else:
            self.cursor = get_test_cursor(self._options)
            close_cursor = True
        # Some tests may not need an adm_cursor so no default for this
        self.adm_cursor = adm_cursor

        try:
            fn_result = self._run_callable(parent_flow_id, capture_stdout)
        except TestFailure as exc:
            result, result_message = self._handle_test_failure(exc, parent_flow_id)
        except cxo.DatabaseError as exc:
            result, result_message = self._handle_cxo_error(exc, parent_flow_id)
        except Exception as exc:
            result, result_message = self._handle_test_exception(exc, parent_flow_id)
        finally:
            try:
                if close_cursor:
                    self.cursor.close()
                    self.cursor.connection.close()
                    if self.adm_cursor:
                        self.adm_cursor.close()
                        self.adm_cursor.connection.close()
            except:
                pass
            reset_tty()

        if not result and not quiet:
            result = 'Pass'
            if self.response_key:
                bench_result_message = self._bench_result_message()
                if bench_result_message:
                    result_message += bench_result_message

        if self._options.teamcity:
            if capture_stdout:
                self._emit_teamcity_stdout(parent_flow_id)
        elif not quiet or (quiet and result):
            global test_lines
            self._bench_result_header(test_lines)
            self._emit_non_teamcity_test_result(result, result_message)
            test_lines += 1

        # Add results to result 'analyzer'
        if self._options.summary and result:
            g_test_results.add_test(self.name, normalize_state(result), result_message)

        if parent_flow_id:
            test_teamcity_endtest_pq(self._options, self.tc_name, parent_flow_id)
        else:
            test_teamcity_endtest(self._options, self.tc_name)

        return fn_result

    def _handle_cxo_error(self, exc, parent_flow_id):
        result = 'Database Exception %s:' % type(exc)
        result_message = '\nSQL:\n%s\nwith binds:\n%s\n\n%s' % (self.cursor.statement,
                                                                dense.pformat(self.cursor.bindvars),
                                                                traceback.format_exc())
        if parent_flow_id:
            test_teamcity_failtest_pq(self._options, self.tc_name, result_message)
        else:
            test_teamcity_failtest(self._options, self.tc_name, result_message)
        return result, result_message


def ora_desc_type_to_cx_type(str_type):
    type_map = {'NUMBER': cxo.NUMBER,
                'RAW': cxo.BINARY,
                'LONG RAW': cxo.LONG_BINARY,
                'LONG': cxo.LONG_STRING,
                'CHAR': cxo.FIXED_CHAR,
                'NCHAR': cxo.FIXED_NCHAR if hasattr(cxo, 'FIXED_NCHAR') else cxo.FIXED_UNICODE,
                'VARCHAR2': cxo.STRING,
                'NVARCHAR2': cxo.NCHAR if hasattr(cxo, 'NCHAR') else cxo.UNICODE,
                'DATE': cxo.DATETIME,
                'TIMESTAMP': cxo.TIMESTAMP,
                'BINARY_DOUBLE': cxo.NATIVE_FLOAT,
                'BINARY_FLOAT': cxo.NATIVE_FLOAT,
                'FLOAT': cxo.NUMBER}

    if 'TIMESTAMP' in str_type:
        return cxo.TIMESTAMP
    elif ORACLE_INTERVAL_DS_RE.match(str_type):
        return cxo.INTERVAL
    cx_type = type_map.get(str_type)
    if not cx_type:
        log('Unable to map DESC type %s to cx_Oracle type' % str_type)
    return cx_type


def bind_to_python_obj(bind):
    if bind['value'] is None:
        return None
    if bind['type'] in ('VARCHAR2', 'CHAR', 'NVARCHAR2', 'NCHAR'):
        return bind['value']
    if 'TIMESTAMP' in bind['type']:
        return dateutil.parser.parse(bind['value'])
    if 'NUMBER' == bind['type']:
        return Decimal(bind['value'])
    if 'DATE' == bind['type']:
        return dateutil.parser.parse(bind['value'])
    if bind['type'] in ('BINARY_FLOAT', 'BINARY_DOUBLE'):
        return float(bind['value'])
    raise Exception('bind_to_python_obj: don\'t know how to convert %s to python object' % bind)


def NumbersAsDecimal(cursor, name, defaultType, size, precision, scale):
    if defaultType == cxo.NUMBER:
        return cursor.var(str, 100, cursor.arraysize, outconverter=Decimal)


def StringsAsUnicode(cursor, name, default_type, size, precision, scale):
    if default_type in (cxo.STRING, cxo.FIXED_CHAR):
        return cursor.var(str, size, cursor.arraysize)


# cx_Oracle 7.3.0 can't bind negative longs of 38 digits. Workaround is to set the bind up
# as a var of cxo.STRING and use an input type handler/converter to do the conversion. Because
# cx_Oracle doesn't automatically convert Long to STRING, it will call the handler to do it...
def WorkaroundLargeNumbersConverter(value):
    return str(value)


def WorkaroundLargeNumbers(cursor, value, arraysize):
    if type(value) == int:
        return cursor.var(cxo.STRING, inconverter=WorkaroundLargeNumbersConverter)


def get_oracle_test_value(proj, base, table_name, column_name, data_type, cursor):
    log('get_oracle_test_value: %s' % proj, detail=verbose)
    cursor.outputtypehandler = NumbersAsDecimal
    extra_pred = " AND {col} != 'Nan' AND {col} != 'Inf'".format(col=column_name) if 'BINARY' in data_type else ''
    if ORACLE_INTERVAL_DS_RE.match(data_type):
        sql_proj = "TO_CHAR({})".format(proj)
    elif ORACLE_INTERVAL_YM_RE.match(data_type):
        sql_proj = "TO_CHAR({})".format(proj)
    elif ORACLE_TIMESTAMP_TZ_RE.match(data_type):
        m = ORACLE_TIMESTAMP_TZ_RE.match(data_type)
        ff_scale = m.group(1)
        sql_proj = "TO_CHAR({},'YYYY-MM-DD HH24:MI:SS.FF{} TZH:TZM')".format(proj, ff_scale)
    else:
        sql_proj = proj
    q = 'SELECT %s FROM %s.%s WHERE %s IS NOT NULL%s' % (sql_proj, base, table_name, column_name, extra_pred)
    log('get_oracle_test_value SQL: %s' % q, detail=verbose)
    row = cursor.execute(q).fetchone()
    literal_or_val = row[0] if row else None
    log('get_oracle_test_value literal: (%s) %s' % (type(literal_or_val), literal_or_val), detail=verbose)
    return literal_or_val


def execute_linux_command(linux, cmd, environment=None):
    assert isinstance(linux, LinuxCmd) and cmd

    if not linux.execute(cmd, environment=environment):
        log("Command: %s failed" % cmd, ansi_code="red")

    log("STDOUT: %s" % linux.stdout, detail=VERBOSE)
    log("STDERR: %s" % linux.stderr, detail=VERBOSE)


class LinuxCmdTest(Test):
    def __init__(self, name, cmd, environment, options):
        self._linux = LinuxCmd()

        def test_fn(test):
            execute_linux_command(self._linux, cmd)

            if name.startswith('fail_'):
                self.assertTrue(self._linux.returncode != 0, \
                    'Linux command: %s returned successful status: %d while failure was expected' % (cmd, self._linux.returncode))
            else:
                self.assertEqual(0, self._linux.returncode, \
                    'Linux command: %s returned non-zero exit status: %d' % (cmd, self._linux.returncode))

        Test.__init__(self, options, name, test_fn)


def run_shell_commands_from_yaml_test(test_definition_file, test_name_re, options):
    def test_generator(name, cmd, try_options, readonly):
        mod_name, mod_cmd = name, cmd

        if try_options:
            if "exactly-once" == try_options:
                yield name, cmd
            else:
                for opt in try_options:
                    mod_name = "%s%s" % (name, opt.replace('-', '_').replace(' ', '_'))
                    mod_cmd = "%s %s" % (cmd, opt)

                    if readonly:
                        mod_name += '_ro'
                    else:
                        mod_cmd += ' -x'

                    yield mod_name, mod_cmd


    def exec_test(linux, env, name, cmd, pre_cmd, try_options, options, readonly):
        for name, cmd in test_generator(name, cmd, try_options, readonly):
            if pre_cmd:
                execute_linux_command(linux, pre_cmd, environment=env)
            t = LinuxCmdTest(name, cmd, environment=env, options=options)
            t()


    source = load_yaml_in_order(test_definition_file)
    data, defaults = source['tests'], source['defaults']
    linux = LinuxCmd()
    env = {
        'CLOUD_SYNC_HOST_ID': os.environ.get('HOSTNAME', 'empty').replace('-', '_').replace('.', '_'),
        'CLOUD_SYNC_MANIFEST_FILE': '../config/tests/offload/cloud_sync_tests.Manifest'
    }
    for k in env:
        os.environ[k] = env[k]

    for test_name in data:
        if options.filter and not re.match(test_name_re, test_name):
            continue

        test_val = data.get(test_name)
        if not isinstance(test_val, dict):
            test_val = {'cmd': test_val}

        test_cmd = test_val.get('cmd')
        pre_cmd = test_val.get('pre')

        try_options = test_val.get('try-options') or defaults.get('try-options')
        add_options = test_val.get('add-options') or defaults.get('add-options')
        skip_cmd_if_ro = test_val.get('skip-cmd-if-ro') or defaults.get('skip-cmd-if-ro')
        skip_pre_if_ro = test_val.get('skip-pre-if-ro') or defaults.get('skip-pre-if-ro')

        if add_options:
            test_cmd = "%s %s" % (test_cmd, " ".join(add_options))

        # First pass - readonly
        if not ("exactly-once" == try_options or skip_cmd_if_ro):
            exec_test(linux, env, test_name, test_cmd, None if skip_pre_if_ro else pre_cmd, try_options, options, readonly=True)
        # Second pass - execute for real ("with -x")
        exec_test(linux, env, test_name, test_cmd, pre_cmd, try_options, options, readonly=False)


def run_cloud_sync_tests(options, test_name_re):
    os.environ['REMOTE_OFFLOAD_CONF'] = options.cloud_sync_remote_offload_config
    return run_shell_commands_from_yaml_test(options.cloud_sync_test_file, test_name_re, options)


# TODO Get rid of ordered and ascii and use the source constants instead
ascii7 = test_constants.TEST_GEN_DATA_ASCII7
ascii7_nonull = test_constants.TEST_GEN_DATA_ASCII7_NONULL
ordered = test_constants.TEST_GEN_DATA_ORDERED
ordered_ascii7 = test_constants.TEST_GEN_DATA_ORDERED_ASCII7
ordered_ascii7_nonull = test_constants.TEST_GEN_DATA_ORDERED_ASCII7_NONULL


def gen_varchar(row_index, length, const=None, from_list=None):
    ordered_flag = ascii7_flag = notnull_flag = False
    if const in [ordered, ordered_ascii7, ordered_ascii7_nonull]:
        ordered_flag = True
    if const in [ascii7, ascii7_nonull, ordered_ascii7_nonull, ordered_ascii7]:
        ascii7_flag = True
    if const in [ascii7_nonull, ordered_ascii7_nonull]:
        notnull_flag = True
    return gen_test_data.gen_varchar(row_index, length, from_list=from_list, ordered=ordered_flag,
                                     ascii7_only=ascii7_flag, notnull=notnull_flag, no_newlines=options.no_newlines)


def gen_char(row_index, length, const=None, from_list=None):
    ordered_flag = ascii7_flag = notnull_flag = False
    if const in [ordered, ordered_ascii7, ordered_ascii7_nonull]:
        ordered_flag = True
    if const in [ascii7, ascii7_nonull, ordered_ascii7_nonull, ordered_ascii7]:
        ascii7_flag = True
    if const in [ascii7_nonull, ordered_ascii7_nonull]:
        notnull_flag = True
    return gen_test_data.gen_char(row_index, length, from_list=from_list, ordered=ordered_flag,
                                  ascii7_only=ascii7_flag, notnull=notnull_flag, no_newlines=options.no_newlines)


def gen_bytes(row_index, length):
    return gen_test_data.gen_bytes(row_index, length)


def gen_uuid(row_index):
    return gen_test_data.gen_uuid(row_index)


def gen_number(row_index, precision=None, scale=0, from_list=None):
    return gen_test_data.gen_number(row_index, precision=precision, scale=scale, from_list=from_list)


def gen_int(row_index, precision=None, from_list=None):
    return gen_test_data.gen_int(row_index, precision=precision, from_list=from_list)


def gen_number_for_oracle_float(row_index, from_list=None):
    return gen_number(row_index, from_list=from_list)


def gen_datetime(row_index, const=None, from_list=None):
    return gen_test_data.gen_datetime(row_index, ordered=bool(const == ordered), from_list=from_list)


def gen_date(row_index, const=None, from_list=None):
    return gen_test_data.gen_date(row_index, ordered=bool(const == ordered), from_list=from_list)


def gen_timestamp(row_index, const=None, from_list=None):
    return gen_test_data.gen_timestamp(row_index, ordered=bool(const == ordered), from_list=from_list)


def gen_time(row_index, const=None, from_list=None):
    return gen_test_data.gen_time(row_index, ordered=bool(const == ordered), from_list=from_list)


def gen_interval_ym(row_index, precision=9):
    return gen_test_data.gen_interval_ym(precision=precision)


def gen_interval_ds(row_index, precision=9, scale=9):
    return gen_test_data.gen_interval_ds(precision=precision, scale=scale)


def gen_float(row_index, from_list=None):
    return gen_test_data.gen_float(row_index, from_list=from_list,
                                   allow_nan=bool(options.target != DBTYPE_SYNAPSE),
                                   allow_inf=bool(options.target != DBTYPE_SYNAPSE))


def gen_canonical_data(row_index, canonical_column, constant=None, literals=None, **kwargs):
    """ Generate data for a column spec described in a generated_tables entry by canonical settings.
    """
    col_type = canonical_column.data_type
    if col_type == GLUENT_TYPE_DATE:
        return gen_date(row_index, const=constant, from_list=literals)
    elif col_type in [GLUENT_TYPE_TIMESTAMP, GLUENT_TYPE_TIMESTAMP_TZ]:
        return gen_datetime(row_index, const=constant, from_list=literals)
    elif col_type == GLUENT_TYPE_TIME:
        return gen_time(row_index, const=constant, from_list=literals)
    elif col_type in [GLUENT_TYPE_VARIABLE_STRING, GLUENT_TYPE_LARGE_STRING]:
        # If we have no length (such as unbound string) then cap the generated values at 32 (why 32? no reason).
        # Added a cap of 500 characters below to remain comfortably under size of Oracle V$SQL.BIND_DATA being RAW(2000)
        # Any checks for larger data should be in sql-offload tests and not tables used for predicates.
        return gen_varchar(row_index, min([canonical_column.data_length or canonical_column.char_length or 32, 500]),
                           const=constant, from_list=literals)
    elif col_type == GLUENT_TYPE_FIXED_STRING:
        return gen_char(row_index, canonical_column.data_length or canonical_column.char_length,
                        const=constant, from_list=literals)
    elif col_type == GLUENT_TYPE_INTEGER_1:
        return gen_int(row_index, canonical_column.data_precision or 2, from_list=literals)
    elif col_type == GLUENT_TYPE_INTEGER_2:
        return gen_int(row_index, canonical_column.data_precision or 4, from_list=literals)
    elif col_type == GLUENT_TYPE_INTEGER_4:
        return gen_int(row_index, canonical_column.data_precision or 9, from_list=literals)
    elif col_type == GLUENT_TYPE_INTEGER_8:
        return gen_int(row_index, canonical_column.data_precision or 16, from_list=literals)
    elif col_type == GLUENT_TYPE_INTEGER_38:
        return gen_number(row_index, precision=canonical_column.data_precision or 36,
                          scale=0, from_list=literals)
    elif col_type == GLUENT_TYPE_DECIMAL:
        return gen_number(row_index, precision=canonical_column.data_precision or 36,
                          scale=canonical_column.data_scale, from_list=literals)
    elif col_type == GLUENT_TYPE_DOUBLE:
        # Not using gen_float in order to avoid inf and nan
        # 15-16 significant digits
        return gen_number(row_index, precision=15, scale=5, from_list=literals)
    elif col_type == GLUENT_TYPE_FLOAT:
        # Not using gen_float in order to avoid inf and nan
        # 7-8 significant digits
        return gen_number(row_index, precision=7, scale=3, from_list=literals)
    elif col_type in [GLUENT_TYPE_BINARY, GLUENT_TYPE_LARGE_BINARY]:
        if literals:
            return literals[(row_index + 1) % len(literals)]
        # To allow testing of backends with SQL that don't support binds we need to keep these values simple
        return gen_uuid(row_index)
    else:
        log('Attempt to generate data for unsupported type: %s' % str(canonical_column))


def gen_data(row_index, type_spec, from_list=None):
    """ Generate data for a column spec described in generated_tables global.
        type_spec is either a scalar data type or a list containing the data type and other attributes.
        type_spec attributes when it is a list:
            [numeric data type, precision, scale, [list, of, int/float, values]]
            [character data type, length, constant which can be ordered or ascii7, ['list', 'of', 'values']]
            [date data type, constant which can be ordered or ascii7, [list, of, datetime, values]]
    """
    if type(type_spec) != tuple:
        type_spec = (type_spec,)
    col_type, ps = type_spec[0], type_spec[1:]
    ps = (row_index,) + ps
    if from_list:
        ps = ps + (from_list,)

    if col_type in [ORACLE_TYPE_VARCHAR2, ORACLE_TYPE_VARCHAR, ORACLE_TYPE_NVARCHAR2]:
        return gen_varchar(*ps)
    elif col_type in [ORACLE_TYPE_CHAR, ORACLE_TYPE_NCHAR]:
        return gen_char(*ps)
    elif col_type == ORACLE_TYPE_NUMBER:
        return gen_number(*ps)
    elif col_type == ORACLE_TYPE_DATE:
        return gen_datetime(*ps)
    elif col_type in [ORACLE_TYPE_TIMESTAMP, ORACLE_TYPE_TIMESTAMP_TZ]:
        return gen_timestamp(*ps)
    elif col_type == ORACLE_TYPE_INTERVAL_YM:
        return gen_interval_ym(*ps)
    elif col_type == ORACLE_TYPE_INTERVAL_DS:
        return gen_interval_ds(*ps)
    elif 'INTERVAL YEAR(9) TO MONTH' in col_type:
        return gen_interval_ym(*ps)
    elif 'INTERVAL DAY(9) TO SECOND(9)' in col_type:
        return gen_interval_ds(*ps)
    elif col_type == ORACLE_TYPE_BINARY_DOUBLE:
        return gen_float(*ps)
    elif col_type == ORACLE_TYPE_BINARY_FLOAT:
        return gen_float(*ps)
    elif col_type == ORACLE_TYPE_FLOAT:
        return gen_number_for_oracle_float(*ps)
    elif col_type in (ORACLE_TYPE_CLOB, ORACLE_TYPE_NCLOB):
        # Add a length while retaining any constant in the tuple
        ps = (ps[0],) + (32768,) + ps[1:]
        return gen_varchar(*ps)
    elif col_type == ORACLE_TYPE_BLOB:
        return gen_varchar(*(row_index, 32768))
    elif col_type == ORACLE_TYPE_RAW:
        if type_spec[1] == 16:
            return gen_uuid(*(row_index,))
        elif len(ps) > 1:
            return gen_bytes(*(row_index, ps[1]))
        else:
            return gen_bytes(*(row_index, 2000))
    else:
        log('Attempt to generate data for unsupported RDBMS type: %s' % str(type_spec))


def gen_cxo_type_spec(type_spec):
    if isinstance(type_spec, tuple):
        scale = None
        length = None
        if len(type_spec) == 3:
            col_type, precision, scale = type_spec[0], type_spec[1], type_spec[2]
        else:
            col_type, precision = type_spec[0], type_spec[1]
            length = precision
    else:
        col_type = type_spec
        precision = scale = length = None
    return gen_test_data.gen_cxo_type_spec(col_type, length, precision, scale)


def data_type_str(type_spec):
    if type(type_spec) != tuple:
        type_spec = (type_spec,)
    col_type, ps = type_spec[0], type_spec[1:]
    # We can have a list of literals on the end of colspec.
    # Remove it if it is there.
    if ps and isinstance(ps[-1], (list, tuple)):
        ps = tuple(ps[:-1])

    type_extra = ''
    if 'CHAR' in col_type:
        type_extra = '(%s)' % str(ps[0])
    elif 'LOB' in col_type or col_type == ORACLE_TYPE_DATE:
        pass
    elif ps and ps[0] is not None and col_type == ORACLE_TYPE_TIMESTAMP:
        type_extra = '(%s)' % ps[0]
    elif ps and ps[0] is not None and col_type == ORACLE_TYPE_TIMESTAMP_TZ:
        return ORACLE_TIMESTAMP_TZ_SUB_PATTERN % str(ps[0])
    elif col_type == ORACLE_TYPE_INTERVAL_DS:
        return ORACLE_INTERVAL_DS_SUB_PATTERN % (9 if ps[0] is None else ps[0],
                                                 9 if len(ps) == 1 or ps[1] is None else ps[1])
    elif col_type == ORACLE_TYPE_INTERVAL_YM:
        return ORACLE_INTERVAL_YM_SUB_PATTERN % (9 if ps[0] is None else ps[0])
    elif ps and (ps[0] is not None or (len(ps) > 1 and ps[1] is not None)):
        type_extra = '(%s)' % ','.join([str(i) if i is not None else '*' for i in ps])

    return '%s%s' % (col_type, type_extra)


def col_sig(name, type_spec):
    col_type = data_type_str(type_spec)
    return '%s %s' % (name, col_type)


def gen_column_types(columns, id_type='NUMBER(18)'):
    return [id_type] + columns


def gen_column_names(columns):
    return ['ID' if num == 0 else 'COLUMN_%s' % num for num in range(len(columns) + 1)]


def ora_partition_exists(c, table_name, partition_name):
    log('ora_partition_exists: %s %s' % (table_name, partition_name), verbose)
    owner, table = table_name.split('.')
    q = 'SELECT 1 FROM dba_tab_partitions WHERE table_owner = UPPER(:o) AND table_name = UPPER(:t) AND partition_name = UPPER(:p)'
    row = c.execute(q, {'o': owner, 't': table, 'p': partition_name}).fetchone()
    return bool(row and row[0] == 1)


def database_charset(options):
    q = "SELECT value FROM v$nls_parameters WHERE parameter = 'NLS_CHARACTERSET'"
    return get_test_cursor(options).execute(q).fetchone()[0]


def oracle_version(options):
    return get_test_cursor(options).connection.version


def oracle_version_number(options):
    version = oracle_version(options)
    stripped = version.replace('.', '')
    return int(stripped)


def create_table(name, rows, columns=[('CHAR', 100), 'NUMBER', 'DATE', 'TIMESTAMP'],
                 create_clause='', column_names=None):
    global options
    naughty_state = get_naughty_state()
    random.setstate(naughty_state)

    log('create_table for %s' % name)
    log('create_table seed/state:\n%s' % str(random.getstate()), detail=vverbose)
    log('create_table rows: %s' % rows, detail=verbose)
    log('create_table columns: %s' % str(columns), detail=verbose)
    if column_names:
        log('create_table column_names: %s' % str(column_names), detail=verbose)

    c = get_test_cursor(options, threaded=False)
    try:
        c.execute('DROP TABLE %s' % name)
        log('%s dropped' % name)
    except:
        log('%s not found' % name)

    coltypes = gen_column_types(columns)
    if column_names:
        colnames = ['ID'] + column_names
    else:
        colnames = gen_column_names(columns)
    cols = [col_sig(colname, spec) for colname, spec in zip(colnames, coltypes)] +\
           generated_tables_extra_cols.get(name.lower().split('.')[1], [])
    q = 'CREATE TABLE %s\n( %s) %s' % (name.upper(), '\n, '.join(cols), create_clause)
    log(q)
    c.execute(q)
    log('%s created' % name)

    bindnames = []
    for i, ctype in zip(list(range(len(colnames))), coltypes):
        bindnames.append(':%s' % i)
    q = 'INSERT INTO %s (%s) VALUES (%s)' % (name.upper(), ', '.join(colnames), ', '.join(bindnames))
    log(q)
    c.prepare(q)

    c.setinputsizes(*[gen_cxo_type_spec(ts) for ts in coltypes])

    #GOE-1503: Following workaround now disabled as we've temporarily shrunk GL_TYPES.NUMBER_16 to a NUMBER(36)
    #c.inputtypehandler = WorkaroundLargeNumbers # see function for why we're using this

    d = [[ri] + [gen_data(ri, col) for col in coltypes[1:]] for ri in range(int(rows))]

# We are seeing this error when using executemany():
# cx_Oracle.DatabaseError: ORA-01458: invalid length inside variable character string
#    c.executemany(None, d)
    for stmt in d:
        c.execute(None, stmt)
    c.connection.commit()

    log('%s inserted: %s' % (rows, coltypes))


def create_select(dest_table, source_table, row_limit=None):
    global options
    c = get_test_cursor(options)
    try:
        c.execute('DROP TABLE %s' % dest_table.upper())
        log('%s dropped' % dest_table)
    except:
        log('%s not found' % dest_table)

    row_limit_clause = ('WHERE ROWNUM <= %s' % row_limit) if row_limit else ''
    sql = 'CREATE TABLE %s AS SELECT * FROM %s %s' % (dest_table.upper(), source_table, row_limit_clause)
    log(sql, verbose)
    c.execute(sql)
    log('%s created as SELECT from %s' % (dest_table, source_table))


def gather_table_stats(options, owner_table):
    log('Gathering table stats for %s' % owner_table)
    owner, table_name = owner_table.split('.') if '.' in owner_table else ('', owner_table)
    get_test_cursor(options).execute("BEGIN DBMS_STATS.GATHER_TABLE_STATS('%s', '%s'); END;" % (owner, table_name))


def create_functions(options, owner_table, columns):
    c = get_test_cursor(options, threaded=False)
    ca = get_test_cursor(options, threaded=False, admin=True)

    colnames = gen_column_names(columns)
    owner, table = owner_table.split('.')
    fnames = []
    for colname in colnames:

        # First function...
        fname = '%s_%s_f1' % (table, colname.lower())
        ddl = """
        CREATE OR REPLACE FUNCTION %(f)s ( p_%(c)s IN %(t)s.%(c)s%%TYPE )
            RETURN %(t)s.%(c)s%%TYPE
            AUTHID CURRENT_USER IS
        BEGIN
            RETURN p_%(c)s;
        END;
        """ % {'f': fname, 't': table, 'c': colname.lower()}
        log(ddl)
        c.execute(ddl)
        log('Function %s created' % fname)
        fnames.append(fname)

        # Second function...
        fname = '%s_%s_f2' % (table, colname.lower())
        ddl = """
        CREATE OR REPLACE FUNCTION %(f)s ( p_%(c)s IN %(t)s.%(c)s%%TYPE )
            RETURN %(t)s.%(c)s%%TYPE
            AUTHID CURRENT_USER IS
            v_%(c)s %(t)s.%(c)s%%TYPE;
        BEGIN
            SELECT %(c)s
            INTO   v_%(c)s
            FROM   %(t)s
            WHERE  %(c)s = p_%(c)s
            AND    ROWNUM = 1;
            RETURN v_%(c)s;
        END;
        """ % {'f': fname, 't': table, 'c': colname.lower()}
        log(ddl)
        c.execute(ddl)
        log('Function %s created' % fname)
        fnames.append(fname)

    #for fname in fnames:
    #    c.execute('GRANT EXECUTE ON %s TO %s' % (fname, get_hybrid_schema(owner)))
    #    ca.execute('CREATE OR REPLACE SYNONYM %s.%s FOR %s.%s' % (get_hybrid_schema(owner), fname, owner, fname))


def get_query_key(query_monitor=None):
    return QueryHintBuilder().unique_hint().query_monitor(query_monitor).str()


def row_count(test, desc, table_name, parallel=0, query_monitor=None):
    hint = QueryHintBuilder().parallel(parallel if parallel else 'SELECT').query_monitor(query_monitor)

    q = 'SELECT ' + hint.str()
    query_key = get_query_key() if options.unique_hint else ''
    q += ' %s COUNT(*) FROM %s' % (query_key, table_name.upper())
    bench_key = '%s_rc_response_time' % test.name
    try:
        result = response_time_bench(test, desc, bench_key, lambda: test.cursor.execute(q).fetchone()[0])
        return result
    except cxo.DatabaseError as exc:
        test.fail(desc + ' failed with:\n' + str(exc) + '\nQuery: ' + q)


def oracle_median_expr(column_name, colspec):
    t = colspec[0]
    # Using PERCENTILE_DISC: it works with date/timestamp and doesn't split difference when equal number of input values
    median_expr_template = 'PERCENTILE_DISC(0.5) WITHIN GROUP (ORDER BY %s) OVER ()'
    if 'CHAR' in t:
        ascii_expr = 'ASCII(SUBSTR(%s, 0, 1))' % column_name
        median_expr = median_expr_template % ascii_expr
        if t.startswith('NCHAR') or t.startswith('NVARCHAR'):
            return 'CHR(%s USING NCHAR_CS)' % median_expr
        else:
            return 'CHR(%s)' % median_expr
    else:
        return median_expr_template % column_name


def oracle_literal(val, data_type=None):
    if isinstance(val, datetime):
        if val.tzinfo:
            return "TIMESTAMP'%s'" % val
        else:
            return "TO_DATE('%04d-%02d-%02d %02d:%02d:%02d', 'yyyy-mm-dd hh24:mi:ss')"\
                   % (val.year, val.month, val.day, val.hour, val.minute, val.second)
    elif val is None:
        return 'NULL'
    elif data_type and ORACLE_TIMESTAMP_TZ_RE.match(data_type):
        return "TIMESTAMP'%s'" % val
    elif isinstance(val, str):
        return "q'[%s]'" % val
    elif data_type and is_nan_capable(data_type) and val and math.isnan(val):
        return "CAST('NaN' AS %s)" % data_type
    elif isinstance(val, float):
        return repr(val)
    else:
        return str(val)


def op_minus_row_count(test, backend_api, desc, source_table, ext_table, column_name, colspec, op, val_expr_fn, use_binds,
                       extra_pred=None):
    q = ''
    bench_key = '%s_opmin_response_time' % test.name
    try:
        if inspect.isfunction(val_expr_fn):
            base, table_name = source_table.split('.')
            val = get_oracle_test_value(val_expr_fn(column_name, colspec), base, table_name, column_name, colspec[0],
                                         test.cursor.connection.cursor())
            if not use_binds:
                val = oracle_literal(val)
        else:
            val = val_expr_fn
        hints = 'MONITOR '
        if use_binds:
            q = 'SELECT /*+ %(hints)s */ COUNT(*) FROM (SELECT %(c)s FROM %(s)s WHERE %(c)s %(op)s :val MINUS SELECT %(c)s FROM %(e)s WHERE %(c)s %(op)s :val)' % {'hints': hints, 'c': column_name, 's': source_table, 'e': ext_table, 'op': op}
            log('Test query:' + q, detail=verbose)

            def minus_query_fn():
                return response_time_bench(test, desc, bench_key,
                                           lambda: test.cursor.execute(q, {'val': val}).fetchone()[0])
        else:
            extra_pred = extra_pred or ''
            q = 'SELECT /*+ %(hints)s */ COUNT(*) FROM (SELECT %(c)s FROM %(s)s WHERE %(c)s %(op)s %(val)s %(extra_pred)s MINUS SELECT %(c)s FROM %(e)s WHERE %(c)s %(op)s %(val)s %(extra_pred)s)' % {'hints': hints, 'c': column_name, 's': source_table, 'e': ext_table, 'op': op, 'val': val, 'extra_pred': extra_pred}
            log('Test query:' + q, detail=verbose)

            def minus_query_fn():
                return response_time_bench(test, desc, bench_key,
                                           lambda: test.cursor.execute(q).fetchone()[0])
        return_count = backend_api.transient_error_rerunner(minus_query_fn)

        return return_count
    except cxo.DatabaseError as exc:
        test.fail(desc + ' failed with:\n' + str(exc) + '\nQuery: ' + q)


def get_hybrid_table_name(table_name):
    return '%s_H' % table_name


def get_hybrid_schema(base_schema):
    return to_hybrid_schema(base_schema)


global generated_tables, generated_views, generated_tables_extra_cols, generated_tables_backend
# entries are either a string for an existing table to offload and test, two strings, for a test table name and
# a source table/view name, or a list for generated-data tables [name, rows, col-spec]
generated_tables, generated_views, generated_tables_extra_cols, generated_tables_backend = [], [], {}, []


# construct tests in functions to ensure proper closure variable capture
def row_count_test_f(base_schema, table_name, parallel=0):
    desc = 'Source/Hybrid row count %s' % table_name
    base = '%s.%s' % (base_schema, table_name)
    hybrid = '%s.%s' % (get_hybrid_schema(base_schema), table_name)
    return lambda test: test.assertEqual(row_count(test, desc, base, parallel),
                                         row_count(test, desc, hybrid, parallel), desc)


def get_suitable_granularity_options(owner, table_name, options):
    """ Returns appropriate partition options for a test offload, of this format:
        (
            partition column csv,
            granularity csv,
            range lower bound,
            range upper bound
        )
        In most cases we return partition columns as None and let offload sort it out but
        for multi-column partitioning on BigQuery we pick the first as a default.
    """
    config = OrchestrationConfig.from_dict({'verbose': options.verbose})
    messages = OffloadMessages.from_options(options, get_log_fh())
    return get_suitable_offload_granularity_options(owner, table_name, config, messages)


def get_suitable_partition_name(owner, table_name, options):
    config = OrchestrationConfig.from_dict({'verbose': options.verbose})
    messages = OffloadMessages.from_options(options, get_log_fh())
    return get_suitable_offload_partition_name(owner, table_name, config, messages)


def change_tz_f(tz):
    desc = 'change tz to %s' % tz
    return lambda test: test.assertEqual(set_tz(tz), True)


def oracle_to_py_charset(cs):
    if 'ISO8859' in cs:
        return 'iso-8859-%s' % re.search('\d+$', cs).group(0)
    elif 'UTF8' in cs:
        return 'utf-8'
    return cs


def get_schema(cursor=None):
    global options
    c = cursor if cursor else get_test_cursor(options)
    if options.source_db_type == 'oracle':
        return c.execute("SELECT SYS_CONTEXT('userenv', 'current_schema') FROM dual").fetchone()[0]
    elif options.source_db_type == 'mssql':
        c.execute("SELECT SCHEMA_NAME()")
        return c.fetchall()[0][0]
    elif options.source_db_type == 'netezza':
        c.execute("SHOW SCHEMA")
        return c.fetchone()[1]


def get_session_user(cursor=None):
    global options
    c = cursor if cursor else get_test_cursor(options)
    if options.source_db_type == 'oracle':
        return c.execute("SELECT SYS_CONTEXT('userenv', 'session_user') FROM dual").fetchone()[0]
    elif options.source_db_type == 'mssql':
        c.execute("SELECT SESSION_USER()")
        return c.fetchall()[0][0]
    elif options.source_db_type == 'netezza':
        c.execute("SELECT SESSION_USER")
        return c.fetchone()[0]


def set_cursor_sharing(cursor, with_sharing: bool):
    if with_sharing:
        sql = 'ALTER SESSION SET CURSOR_SHARING = FORCE'
    else:
        sql = 'ALTER SESSION SET CURSOR_SHARING = EXACT'
    log(sql, detail=verbose)
    cursor.execute(sql)


def set_schema(schema, cursor=None):
    global options
    c = cursor if cursor else get_test_cursor(options)
    c.execute('ALTER SESSION SET CURRENT_SCHEMA = %s' % schema)


def get_tz(cursor=None):
    global options
    c = cursor if cursor else get_test_cursor(options)
    # +00:00, take first 6 chars only because some unprintable char is appended
    return c.execute('SELECT TZ_OFFSET(SESSIONTIMEZONE) FROM dual').fetchone()[0][:6]


def set_tz(tz, cursor=None):
    global options
    c = cursor if cursor else get_test_cursor(options)
    c.execute("ALTER SESSION SET TIME_ZONE='%s'" % tz)
    return tz == get_tz()


def code_point_fn(cs):
    if cs == 'utf-8':
        return lambda cp: chr(cp)
    else:
        return lambda cp: chr(cp).decode(cs)

global code_point_to_char
code_point_to_char = lambda cp: cp


def cols(options, table_spec):
    if len(table_spec) >= 3:
        return list(zip(gen_column_names(table_spec[2]), gen_column_types(table_spec[2])))
    elif len(table_spec) == 2:
        return [(n, (t, l)) for n, t, l, s in desc_columns(get_test_cursor(options), table_spec[1])]


# https://www.bettercodebytes.com/theadpoolexecutor-with-a-bounded-queue-in-python/
class BoundedExecutor:
    """BoundedExecutor behaves as a ThreadPoolExecutor which will block on
    calls to submit() once the limit given as "bound" work items are queued for
    execution.

    :param bound: Integer - the maximum number of items in the work queue
    :param max_workers: Integer - the size of the thread pool
    """
    def __init__(self, bound, max_workers):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.semaphore = BoundedSemaphore(bound + max_workers)

    """See concurrent.futures.Executor#submit"""
    def submit(self, fn, *args, **kwargs):
        self.semaphore.acquire()
        try:
            future = self.executor.submit(fn, *args, **kwargs)
        except:
            self.semaphore.release()
            raise
        else:
            future.add_done_callback(lambda x: self.semaphore.release())
            return future

    """See concurrent.futures.Executor#shutdown"""
    def shutdown(self, wait=True):
        self.executor.shutdown(wait)


class TestRunner:

    def __init__(self, options, max_workers=None):
        if not max_workers:
            max_workers = multiprocessing.cpu_count() * 2 if options.teamcity else 1
        self.executor = BoundedExecutor(64, max_workers=max_workers)

    def execute_test(self, test, *args, **kwargs):
        self.executor.submit(test, *args, **kwargs)

    def wait_for_tests_to_finish(self):
        self.executor.shutdown()


def get_pq_suffix(parallel):
    return f'_pq{parallel}' if parallel else ''


def run_an_offload(op_params, options):
    """Just run an offload."""
    assert op_params and type(op_params) is dict and 'owner_table' in op_params

    log('Offloading:\n' + str(op_params), verbose)

    if 'max_offload_chunk_size' not in op_params:
        # Keep offloads chugging along nicely unless a test asked for a specific value
        op_params['max_offload_chunk_size'] = '1G'

    config_overrides = {'execute': options.execute, 'verbose': options.verbose}
    offload_status = OrchestrationRunner(config_overrides=config_overrides).offload(op_params)
    return offload_status


def is_nan_capable(data_type):
    return bool(data_type in ['BINARY_FLOAT', 'BINARY_DOUBLE'])


def display_backend_stats(db, table):
    divider = '=' * 120
    light_divider = '-' * 120
    divider_long = '=' * 152
    light_divider_long = '-' * 152

    backend_api = get_backend_testing_api(options)
    t, p, c = backend_api.get_table_and_partition_stats(db, table)

    log('', verbose)
    log(divider, verbose)
    log('TABLE STATS', verbose)
    log(divider, verbose)
    log('%-55s %20s %20s %22s' % ('Table', 'Num Rows', 'Bytes', 'Avg Row Len'), verbose)
    log(light_divider, verbose)
    log('%-55s %20s %20s %22s' % (table, t[0], t[1], t[2]), verbose)
    log(divider, verbose)
    log('', verbose)

    log(divider, verbose)
    log('PARTITION STATS', verbose)
    log(divider, verbose)
    log('%-55s %20s %20s %22s' % ('Partition', 'Num Rows', 'Bytes', 'Avg Row Len'), verbose)
    log(light_divider, verbose)
    for part in p:
        log('%-55s %20s %20s %22s' % (part[0], part[1], part[2], part[3]), verbose)
    log(divider, verbose)
    log('', verbose)

    log(divider_long, verbose)
    log('COLUMN STATS', verbose)
    log(divider_long, verbose)
    log('%-55s %-20s %10s %10s %15s %10s %10s %15s'
        % ('Partition', 'Column', 'NDV', 'Nulls', 'Avg Col Len', 'Low Val', 'High Val', 'Max Col Len'), verbose)
    log(light_divider_long, verbose)
    for col in c:
        if len(col) == 8:
            log('%-55s %-20s %10s %10s %15s %10s %10s %15s'
                % (col[0], col[1], col[2], col[3], col[4], col[5], col[6], col[7]), verbose)
        else:
            log('%-55s %-20s %10s %10s %15s %10s %10s %15s'
                % ('None', col[0], col[1], col[2], col[3], col[4], col[5], col[6]), verbose)
    log(divider_long, verbose)
    log('Note: for Hive we only report the first column for performance reasons. Impala shows all columns but only global stats.')
    log('', verbose)


def story_test_f(story, config_options, initial_test_options):
    """
    Partially mirrored by story_runner.story_test_f().
    Ideally we'll retire this code and keep the story_runner version but at the moment it's too difficult due to
    changes on the Console feature branches.
    TODO deal with this duplication.
    """
    def restore_initial_test_options():
        """ Restore the options we copied when we started test
        """
        init(initial_test_options)
        normalise_options(initial_test_options, normalise_owner_table=False)

    if story['type'] == STORY_TYPE_OFFLOAD:
        test_fn = story_runner.get_story_type_offload_fn(story, config_options)

    elif story['type'] == STORY_TYPE_SETUP:
        test_fn = story_runner.get_story_type_setup_fn(story, config_options, options)

    elif story['type'] == STORY_TYPE_AGG_VALIDATE:
        test_fn = story_runner.get_story_type_agg_validate_fn(story, config_options)

    elif story['type'] == STORY_TYPE_PRESENT:
        test_fn = story_runner.get_story_type_present_fn(story, config_options)

    elif story['type'] == STORY_TYPE_SCHEMA_SYNC:
        test_fn = story_runner.get_story_type_schema_sync_fn(story, config_options)

    elif story['type'] == STORY_TYPE_SHOW_BACKEND_STATS:
        def test_fn(test):
            skip_assertions = False
            try:
                display_backend_stats(story['db'], story['table'])
            except Exception as exc:
                expected_exception_string = story.get('expected_exception_string')
                if expected_exception_string and expected_exception_string in str(exc):
                    log('Ignoring expected exception containing "%s"' % expected_exception_string, detail=verbose)
                    skip_assertions = True
                else:
                    log(traceback.format_exc())
                    raise

            if story.get('assertion_pairs') and not skip_assertions:
                for i, (fn1, fn2) in enumerate(story['assertion_pairs']):
                    try:
                        test.assertEqual(fn1(test), fn2(test), '%s (assertion %s)' % (story['title'], i))
                    except Exception as exc:
                        expected_exception_string = story.get('expected_exception_string')
                        if expected_exception_string and expected_exception_string in str(exc):
                            log('Ignoring expected exception containing "%s"' % expected_exception_string,
                                detail=verbose)
                        else:
                            log(traceback.format_exc())
                            raise
    elif story['type'] == STORY_TYPE_SHELL_COMMAND:
        test_fn = story_runner.get_story_type_shell_cmd_fn(story, config_options, options)

    elif story['type'] == STORY_TYPE_PYTHON_FN:
        test_fn = story_runner.get_story_type_python_fn(story)

    return test_fn


def run_story_test_process(options, story_name, should_run_test_f, orchestration_config, initial_test_options, flow_id):
    """
    Mirrored by story_runner.run_story_test_process().
    Ideally we'll retire this code and keep the story_runner version but at the moment it's too difficult.
    TODO deal with this duplication.
    """
    test_name = None
    repo_client = None
    try:
        init_log(story_name)
        messages = OffloadMessages.from_options(options, log_fh=get_log_fh())

        block_flow_id = "%s.%s" % (flow_id, story_name)

        story_runner.run_story_test_process_start_flow(options, story_name, block_flow_id, flow_id)

        test_cursor_factory = lambda: get_test_cursor(options)
        hybrid_cursor_factory = lambda: get_test_cursor(options, hybrid=True)
        repo_client = orchestration_repo_client_factory(options, messages)
        stories = story_runner.get_story_tests(options.test_user, messages, orchestration_config,
                                               story_list=story_name, options=options, repo_client=repo_client,
                                               test_cursor_factory=test_cursor_factory,
                                               hybrid_cursor_factory=hybrid_cursor_factory)

        story_list = stories[story_name]

        story_runner.check_story_integrity(story_name, story_list, options, block_flow_id, messages)

        for story in story_list:
            test_name = story['id']
            if not should_run_test_f(test_name):
                continue

            try:
                test_title = story.get('title', test_name)
                if story.get('type') == STORY_TYPE_LINK_ID:
                    story = story_runner.copy_linked_test_attributes(story, story_list)

                if options.teamcity:
                    print("##teamcity[blockOpened name='%s' description='%s' flowId='%s']" %
                          (test_name, teamcity_escape(test_title), block_flow_id), flush=True)
                    log('\n%s (%s)' % (test_title, test_name), ansi_code='underline', detail=vverbose)
                else:
                    log('\n%s (%s)' % (test_title, test_name), ansi_code='underline')

                if not story_runner.run_story_test_process_prereq(options, story, test_name, block_flow_id):
                    continue

                use_configs = story_runner.run_story_test_process_configs(options, orchestration_config, story)
                t = Test(options, test_name, story_test_f(story, use_configs, initial_test_options))
                t(capture_stdout=options.teamcity, parent_flow_id=block_flow_id)

            finally:
                if options.teamcity:
                    print("##teamcity[blockClosed name='%s' flowId='%s']" %
                          (test_name, block_flow_id), flush=True)

    except Exception as exc:
        log('Aborting story %s due to exception in test %s, further stories will attempt to complete'
            % (story_name, test_name))
        log('Exception:\n%s\n%s' % (str(exc), traceback.format_exc()))
    finally:
        story_runner.run_story_test_process_end_flow(options, story_name, block_flow_id, flow_id)
        if repo_client:
            try:
                repo_client.close()
            except:
                pass


def run_story_tests(options, should_run_test_f, teamcity_name=test_constants.SET_STORIES):
    """
    Mirrored by story_runner.run_story_tests().
    Ideally we'll retire this code and keep the story_runner version but at the moment it's too difficult.
    TODO deal with this duplication.
    """
    flow_id = test_teamcity_starttestsuite_pq(options, teamcity_name)

    try:
        # take a copy of these options so we can restore this state
        initial_test_options = copy.copy(options)
        # One time setup applicable to all subsequent offloads
        # Ensure gluent.py global options reflects the values in test options.
        init(options)
        options.execute = True
        options.verify_row_count = 'minus'
        normalise_options(options, normalise_owner_table=False)

        config = OrchestrationConfig.from_dict({'execute': True,
                                                'verbose': options.verbose,
                                                'vverbose': options.vverbose})

        stories_to_run = story_runner.gen_stories_to_run(options, config, teamcity_name)

        with ProcessPoolExecutor(multiprocessing.cpu_count() if options.teamcity else 1) as pool:
            for story_name in stories_to_run:
                pool.submit(run_story_test_process, options, story_name, should_run_test_f, config,
                            initial_test_options, flow_id)

    finally:
        test_teamcity_endtestsuite_pq(options, teamcity_name, flow_id)


def run_pre_sales_tests(options, should_run_test_f, current_schema):
    test_teamcity_starttestsuite(options, 'pre-sales')

    try:
        offload_home = os.getenv('OFFLOAD_HOME', os.getenv('OFFLOAD_ROOT'))
        assert offload_home is not None, 'environment variable, OFFLOAD_HOME or OFFLOAD_ROOT, must be set to run Pre Sales tests'

        is_teamcity = os.getenv('TEAMCITY_PROJECT_NAME', None)
        if is_teamcity is not None:
            test_script_path = './'
            sql_script_path = '../../pre_sales'
        else:
            test_script_path = '../integration_test'
            sql_script_path = '../sql/oracle/tools/pre-sales'
        test_script_path = os.path.join(os.getcwd(), test_script_path)
        sql_script_path = os.path.join(os.getcwd(), sql_script_path)

        # sql_disco.sql
        teamcity_starttestblock('sql_disco')
        sql_disco_script_path = os.path.join(test_script_path, 'generate_sql_disco.sh')
        generate_sql_disco_cmd = [sql_disco_script_path, sql_script_path, offload_home, options.ora_test_adm_user,
                                  options.ora_test_adm_pass, options.oracle_dsn, '7', 'D', 'CPU', 'SQL', '60', '1']
        log('generate_sql_disco_cmd: %s' % ' '.join(generate_sql_disco_cmd), detail=verbose)
        cmd_status = subprocess.call(generate_sql_disco_cmd)
        assert cmd_status == 0, 'Pre Sales generation script, generate_sql_disco_cmd.sh, returned non-zero exit status'
        teamcity_endtestblock('sql_disco')

        # table_disco.sql
        teamcity_starttestblock('table_disco')
        table_disco_script_path = os.path.join(test_script_path, 'generate_table_disco.sh')
        generate_table_disco_cmd = [table_disco_script_path, sql_script_path, offload_home, options.ora_test_adm_user,
                                    options.ora_test_adm_pass, options.oracle_dsn,
                                    '%s.GL_SALES' % current_schema.upper(), '7', 'ASH']
        log('generate_table_disco_cmd: %s' % ' '.join(generate_table_disco_cmd), detail=verbose)
        cmd_status = subprocess.call(generate_table_disco_cmd)
        assert cmd_status == 0, 'Pre Sales generation script, generate_table_disco_cmd.sh, returned non-zero exit status'
        teamcity_endtestblock('table_disco')

    finally:
        test_teamcity_endtestsuite(options, 'pre-sales')


def get_firefox_version():
    """ Get the firefox version from an os call
    """
    cmd = ['firefox', '--version']
    out = subprocess.check_output(cmd, stderr=subprocess.STDOUT)
    out = out.decode().strip().split('\n')
    return out[-1].split()[-1]


def get_geckodriver_version(geckodriver_path):
    """ Get the Geckodriver version from an os call
    """
    assert geckodriver_path
    cmd = [geckodriver_path, '--version']
    out = subprocess.check_output(cmd, stderr=subprocess.STDOUT)
    out = out.decode().strip().split('\n')
    return out[0].split()[1]


def get_html_driver():
    """ Init the driver for HTML report generation
    """
    firefox_options = Options()
    firefox_options.headless = True
    firefox_options.set_capability('marionette', True)
    firefox_options.set_preference("network.http.spdy.enabled.http2", False)
    geckodriver_base = './' if os.getenv('TEAMCITY_PROJECT_NAME', None) else '../integration_test'
    geckodriver_binary = 'geckodriver.0.30.0'
    geckodriver_path = os.path.join(geckodriver_base, geckodriver_binary)
    service = Service(executable_path=geckodriver_path)
    driver = webdriver.Firefox(options=firefox_options, service=service)
    driver.set_page_load_timeout(10)
    log('=================== HTML Driver Details ===================')
    log('Firefox version    : %s' % str(driver.capabilities['browserVersion']))
    log('Firefox headless   : %s' % firefox_options.headless)
    log('Selenium version   : %s' % webdriver.__version__)
    log('Geckodriver version: %s' % get_geckodriver_version(geckodriver_path))
    log('Geckodriver binary : %s' % geckodriver_path)
    log('===========================================================')
    return driver


def run_diagnose_tests(options, should_run_test_f, current_schema, orchestration_config):
    test_teamcity_starttestsuite(options, 'diagnose')

    def diagnose_test_fn(diag, method, params):
        def test_fn(test):
            try:
                out = getattr(diag, method)(*params)
                if method.startswith('logs_'):
                    if len(out) == 0:
                        test.fail('No files matched')
                    elif len(out) > 2:
                        log('File match: %s' % out[0])
                        log('File match: (<%s more files>)' % int(len(out)-1))
                    else:
                        [log('File match: %s' % f, verbose) for f in out]
                elif method.startswith('remove_files'):
                    filename = os.path.join(options.output_location, 'gluent_diag_test_api.txt')
                    if os.path.isfile(filename):
                        test.fail(filename + ' was not removed')
                else:
                    if out:
                        if method.endswith(('_config', '_query_log', '_application_log')):
                            log(str(out)[-100:])
                        else:
                            log(str(out))
                    else:
                        if not method.startswith('remove_files'):
                            test.fail(method + ' returned no output')
            except Exception as exc:
                test.fail(method + ' failed with:\n' + traceback.format_exc())

        return test_fn

    def get_offload_log_file_name(cursor, orchestration_config, user_sid, extension):
        filename_sql = textwrap.dedent("""\
            SELECT  file_name
            FROM    TABLE({p_gluent_adm}.offload_file.list_offload_log_files(p_sid => {p_sid}))
            WHERE   file_name LIKE '%{p_ext}'
            AND     file_name NOT LIKE '%jfpd%'""".format(p_gluent_adm=orchestration_config.ora_adm_user,
                                                          p_sid=user_sid,
                                                          p_ext=extension))
        log('filename_sql: %s' % filename_sql, detail=VERBOSE)
        file_name = cursor.execute(filename_sql).fetchone()
        return file_name[0] if file_name else None

    try:
        init(options)
        normalise_options(options, normalise_owner_table=False)
        options.log_location = options.output_location = '%s/log' % os.environ['OFFLOAD_HOME']

        messages = OffloadMessages.from_options(options, log_fh=get_log_fh(),
                                                command_type=orchestration_constants.COMMAND_DIAGNOSE)
        diag = Diagnose.from_options(options, messages)

        ahead_1hr = (datetime.now() + timedelta(hours=1)).strftime('%Y-%m-%d_%H:%M:%S')
        behind_1hr = (datetime.now() - timedelta(hours=1)).strftime('%Y-%m-%d_%H:%M:%S')

        filename = os.path.join(options.output_location, 'gluent_diag_test_api.txt')
        with open(filename, 'w') as out_file:
            out_file.write('\nGluent Diagnose Output File')
            out_file.write('\nCopyright 2015-%s Gluent Inc. All rights reserved.' % datetime.now().strftime('%Y'))

        api_calls = [{'method': 'logs_last_n', 'params': [options.log_location, '1h']},
                     {'method': 'logs_from', 'params': [options.log_location, behind_1hr]},
                     {'method': 'logs_to', 'params': [options.log_location, ahead_1hr]},
                     {'method': 'logs_between', 'params': [options.log_location, behind_1hr, ahead_1hr]},
                     {'method': 'processes', 'params': [['offload', 'smon', 'cron']]},
                     {'method': 'permissions', 'params': [PermissionsTuple('directory', os.environ['OFFLOAD_HOME'], False)]},
                     {'method': 'frontend_ddl', 'params': [options.test_user.upper(), 'GL_SALES', 'TABLE']},
                     {'method': 'oracle_table_stats', 'params': [options.test_user.upper(), 'GL_SALES']},
                     {'method': 'oracle_gluent_dependencies', 'params': [options.test_user.upper(), 'GL_SALES']},
                     {'method': 'oracle_rewrite_definition', 'params': ['%s_H' % options.test_user.upper(), 'GL_SALES_AGG']}]

        repo_client = orchestration_repo_client_factory(options, messages)
        backend_db, backend_table = get_backend_db_table_name_from_metadata('%s_H' % options.test_user.upper(),
                                                                            'GL_SALES', repo_client)

        api_calls.extend([{'method': 'backend_ddl', 'params': [backend_db, backend_table]},
                          {'method': 'backend_table_stats', 'params': [backend_db, backend_table]},
                          {'method': 'create_file', 'params': [options.output_location, 'api_test_file',
                                                               ['one two three']]},
                          {'method': 'create_zip_archive', 'params': [options.output_location, [filename]]},
                          {'method': 'remove_files', 'params': [options.output_location, [filename]]}])

        for api_call in api_calls:
            log('Diagnose (%s)' % api_call['method'], ansi_code='underline')
            t = Test(options, api_call['method'], diagnose_test_fn(diag, api_call['method'], api_call['params']))
            t()

    except Exception as exc:
        log('Unhandled exception in diagnose test set:\n%s' % traceback.format_exc())
        test_teamcity_failtest(options, 'run_diagnose_tests', 'Fail: %s' % str(exc))
    finally:
        test_teamcity_endtestsuite(options, 'diagnose')


def run_offload_status_report_tests(options, should_run_test_f, current_schema):
    flow_id = test_teamcity_starttestsuite_pq(options, 'offload_status_report')

    if options.continuous:
        osr_formats = [TEXT, HTML]
        log('Testing reduced OSR formats in continuous mode: %s' % str(osr_formats), detail=verbose)
    else:
        osr_formats = [TEXT, HTML, CSV, JSON, RAW]
    try:
        test_teamcity_starttest_pq(options, 'setup_offload_status_report_tests', flow_id)

        osr_tests = []
        for format in osr_formats:
            for level in [SUMMARY_LEVEL, DETAIL_LEVEL]:
                osr_options = {'test_name': 'osr_%s_%s' % (level, format),
                               'output_level': level,
                               'output_format': format,
                               'report_name': 'Gluent_Offload_Status_Report_%s' % level,
                               'schema': current_schema,
                               'table': 'SALES' if options.continuous else None,
                               }
                osr_tests.append(osr_options)

        normalise_options(options, False)
        if options.ora_test_adm_user == options.ora_adm_user:
            # this password may have been decrypted above so let's take the decrypted version
            options.ora_test_adm_pass = options.ora_adm_pass
        test_teamcity_endtest_pq(options, 'setup_offload_status_report_tests', flow_id)

        with ProcessPoolExecutor(multiprocessing.cpu_count() if options.teamcity else 1) as pool:
            for test in osr_tests:
                if should_run_test_f(test['test_name']):
                    pool.submit(run_offload_status_report_test_process, options, test, flow_id)

    except Exception as exc:
        log('Unhandled exception in offload_status_report test set:\n%s' % traceback.format_exc())
        if options.teamcity:
            test_teamcity_failtest_pq(options, 'setup_offload_status_report_tests', 'Fail: %s' % str(exc),
                                      flow_id=flow_id)
    finally:
        test_teamcity_endtestsuite_pq(options, 'offload_status_report', flow_id)


def run_offload_status_report_test_process(options, test, flow_id):
    t = OffloadStatusReportTest(options, test)
    t(capture_stdout=options.teamcity, parent_flow_id=flow_id)


class OffloadStatusReportTest(Test):
    """
    OffloadStatusReportTest: test report generation for combinations of output_level and output_format for the current_schema.

    We have two test steps in this class:

       1) instantiate and generate an Offload Status Report
       2) assert that expected elements of the report are present
    """

    def __init__(self, options, osr_options):
        test_name = osr_options['test_name']

        # Generate the Offload Status Report
        options.report_name = osr_options.get('report_name', DEFAULT_REPORT_NAME)
        options.report_directory = DEFAULT_REPORT_DIRECTORY
        if osr_options['output_format'] == CSV:
            options.csv_delimiter = '|'
            options.csv_enclosure = '~'
        else:
            options.csv_delimiter = DEFAULT_CSV_DELIMITER
            options.csv_enclosure = DEFAULT_CSV_ENCLOSURE

        def test_fn(test):
            try:
                normalise_offload_status_report_options(options)
                messages = OffloadMessages.from_options(options, log_fh=get_log_fh(),
                                                        command_type=orchestration_constants.COMMAND_OSR)
                orchestration_config = OrchestrationConfig.from_dict({'execute': True,
                                                                      'verbose': options.verbose,
                                                                      'vverbose': options.vverbose})
                osr = OffloadStatusReport(orchestration_config, messages)
                report_data = osr.get_offload_status_report_data(schema=osr_options['schema'],
                                                                 output_level=osr_options['output_level'],
                                                                 table=osr_options['table'])
                status = osr.gen_offload_status_report(report_data, osr_options['output_format'],
                                                       options.report_name, options.report_directory,
                                                       options.csv_delimiter, options.csv_enclosure)
                if status > 0:
                    self.fail('Failed to generate Offload Status Report {report_file}. Exit status code was {exit_code}'.format(report_file=osr._report_file, exit_code=status))
            except Exception as exc:
                log(traceback.format_exc())
                self.fail('Exception generating Offload Status Report: %s' % str(exc))

            # Parse the report and perform assertions
            if osr_options['output_format'] == TEXT:
                report_file = open(osr._report_file, 'r')
                report_text = report_file.read()
                report_file.close()

                def check_report(text, item):
                    self.assertTrue(re.findall(text, report_text), 'check_report: {item} not found in {report_file}'.format(item=item, report_file=osr._report_file))

                # assertions for summary text report
                # TODO: these could be expanded to search for elements within each section
                check_report("1\. Options", 'Options section')
                check_report("2\. Database Environment", 'Database Environment section')
                check_report("3\. Schema Summary", 'Schema Summary section')
                check_report("4\. Table Summary", 'Table Summary section')

                # assertions for detail text report
                if osr_options['output_level'] == 'detail':
                    check_report("5\. Table Detail", 'Table Detail section')

            elif osr_options['output_format'] == HTML:

                with get_html_driver() as driver:

                    def check_xpath(xpath, item):
                        try:
                            driver.find_element(by=By.XPATH, value=xpath)
                        except NoSuchElementException:
                            self.fail('check_xpath: {item} not found in {report_file}'.format(item=item, report_file=osr._report_file))
                        return True

                    def check_percentage_display(xpath, item):
                        try:
                            element = driver.find_element(by=By.XPATH, value=xpath)
                            if len(element.text[:-1].split('.')[1]) != 2:
                                self.fail('check_percentage_display: {item} is greater than 2 decimal places: {percentage}'.format(item=item,
                                                                                                         percentage=element.text))
                        except NoSuchElementException:
                            self.fail('check_percentage_display: {item} not found in {report_file}'.format(item=item, report_file=osr._report_file))
                        return True

                    # Grab the file and process it through firefox ready for inspection
                    driver.get("file://{report_file}".format(report_file=osr._report_file))

                    # assertions for summary html report
                    check_xpath("//div[@id='show_options_container']/div[1]/div[2]/h3[text() = 'Options']", 'Options section')
                    check_xpath("//div[@id='show_options_container']/div[1]/div[2]/h3[text() = 'Database Environment']", 'Database Environment section')
                    check_xpath("//div[@id='osr_gauge_container' and not(contains(@class, 'invisible'))]", 'Data Distribution chart')
                    check_xpath("//div[@id='reclaim_size_chart' and not(contains(@class, 'invisible'))]", 'RDBMS Breakdown (Size) chart')
                    check_xpath("//div[@id='reclaim_partitions_chart' and not(contains(@class, 'invisible'))]", 'RDBMS Breakdown (Segments) chart')
                    check_xpath("//div[@id='reclaim_rows_chart' and not(contains(@class, 'invisible'))]", 'RDBMS Breakdown (Rows) chart')

                    check_percentage_display("//div[@id='osr_gauge_container' and not(contains(@class, 'invisible'))]/div[1]/div/table/tbody/tr[2]/td[1]/div[2]/small", 'Retained Percentage')
                    check_percentage_display("//div[@id='osr_gauge_container' and not(contains(@class, 'invisible'))]/div[1]/div/table/tbody/tr[2]/td[2]/div[2]/small", 'Reclaimable Percentage')
                    check_percentage_display("//div[@id='osr_gauge_container' and not(contains(@class, 'invisible'))]/div[1]/div/table/tbody/tr[2]/td[3]/div[2]/small", 'Offloaded Percentage')

                    check_xpath("//div[@id='partial_panel_body']/div/div[4]/div[1]/abbr/span[text() = '{schema}']".format(schema=current_schema.upper()), 'Schema Summary ({schema} label)'.format(schema=current_schema.upper()))
                    check_xpath("//div[@id='partial_panel_body']/div/div[4]/div[2]/div/div[@class='progress-bar background-retain' and @schema_name='{schema}']".format(schema=current_schema.upper()), 'Schema Summary ({schema} retained progress bar)'.format(schema=current_schema.upper()))
                    check_xpath("//div[@id='partial_panel_body']/div/div[4]/div[2]/div/div[@class='progress-bar background-reclaim' and @schema_name='{schema}']".format(schema=current_schema.upper()), 'Schema Summary ({schema} reclaimable progress bar)'.format(schema=current_schema.upper()))
                    check_xpath("//div[@id='partial_panel_body']/div/div[4]/div[3]/div/div[@class='progress-bar background-offload width-100-pct' and @schema_name='{schema}']".format(schema=current_schema.upper()), 'Schema Summary ({schema} offloaded progress bar)'.format(schema=current_schema.upper()))

                    check_xpath("//div[@id='schemas_1_search']/div[1]/div[1]/span[normalize-space() = '{schema}']".format(schema=current_schema.upper()), 'Table Summary ({schema} label)'.format(schema=current_schema.upper()))
                    check_xpath("//div[@id='schemas_1_search']/div[1]/div[2]/span[1][text() = 'TABLES']", 'Table Summary (TABLES label)')
                    check_xpath("//div[@id='schemas_1_search']/div[1]/div[2]/span[2][text()]", 'Table Summary (TABLES value)')
                    check_xpath("//tbody[@id='tbody_1']//tr/td[@data-table_name = 'SALES']", 'Table Summary (SALES table name)')

                    # assertions for detail html report
                    if osr_options['output_level'] == 'detail':
                        check_xpath("//button[@id='{schema}_SALES_modal_button']".format(schema=current_schema.upper()), 'Table Summary (SALES table detail button)')
                        check_xpath("//div[@id='{schema}_SALES_modal']/div/div/div[1]/h4/abbr/strong[normalize-space() = '{schema}.SALES']".format(schema=current_schema.upper()), 'Table Detail ({schema}.SALES title)'.format(schema=current_schema.upper()))

                    # Save screenshot
                    driver.save_screenshot('{report_file}_screenshot.png'.format(report_file=osr._report_file.split('.')[0]))
                    # Save HTML
                    with open('{report_file}_firefox_output.html'.format(report_file=osr._report_file.split('.')[0]), 'wb') as html_out:
                        html_out.write(driver.page_source.encode('utf-8').strip())

            elif osr_options['output_format'] == CSV:

                # assert that we can read the CSV file using an external table
                cursor = get_test_cursor(options)
                try:
                    ext_table = 'gl_osr_%s_ext' % osr_options['output_level']
                    cursor.execute("ALTER TABLE {table_name} LOCATION ('{location}')".format(table_name=ext_table, location=osr._report_file.split('/')[-1]))
                    rows = cursor.execute(f'SELECT * FROM {ext_table}').fetchall()
                    self.assertTrue(bool(len(rows) > 0), 'Unable to read {report_file}'.format(report_file=osr._report_file))
                    # We know how many partitions should be offloaded from certain auto-generated tables
                    # therefore we can do some better checks here.
                    expected_reclaimable_partitions = {
                        'CHANNELS': 1,
                        PART_LIST_TABLE_DATE.upper(): len(PART_RANGE_DATE_VALUES) - 1,
                        PART_LIST_TABLE_NUM.upper(): len(PART_RANGE_NUM_VALUES) - 1,
                        PART_LIST_AS_RANGE_TABLE_DATE.upper(): len(PART_RANGE_DATE_VALUES) - 1,
                        PART_LIST_AS_RANGE_TABLE_NUM.upper(): len(PART_RANGE_NUM_VALUES) - 1,
                        PART_RANGE_TABLE_DATE_DATE.upper(): len(PART_RANGE_DATE_VALUES) - 1,
                        PART_RANGE_TABLE_DATE.upper(): len(PART_RANGE_DATE_VALUES) - 1,
                        PART_RANGE_TABLE_DEC.upper(): len(PART_RANGE_NUM_VALUES) - 1,
                        test_constants.PART_RANGE_TABLE_UDEC.upper(): len(PART_RANGE_NUM_VALUES) - 1,
                        PART_RANGE_TABLE_NUM.upper(): len(PART_RANGE_NUM_VALUES) - 1,
                        test_constants.PART_RANGE_TABLE_UNUM.upper(): len(PART_RANGE_NUM_VALUES) - 1,
                        PART_RANGE_TABLE_STR.upper(): len(PART_RANGE_STR_HVS),
                        test_constants.PART_RANGE_TABLE_USTR.upper(): len(PART_RANGE_STR_HVS),
                    }
                    sql = f'SELECT source_table, reclaimable_parts FROM {ext_table}'
                    log(f'SQL: {sql}', detail=VVERBOSE)
                    rows = cursor.execute(sql).fetchall()
                    for csv_table, csv_parts in rows:
                        if csv_table.upper() in expected_reclaimable_partitions:
                            log(f'Checking OSR offload partition count for: {csv_table}', detail=VERBOSE)
                            self.assertEqual(csv_parts, expected_reclaimable_partitions[csv_table.upper()],
                                             f'{csv_table} CSV offloaded_parts != expected partition count')
                except cxo.DatabaseError as exc:
                    msg = str(exc).strip()
                    self.fail(f'Unable to read {osr._report_file} due to database error {msg}')
                finally:
                    try:
                        cursor.close()
                        cursor.connection.close()
                    except cxo.OperationalError:
                        pass

            elif osr_options['output_format'] in (JSON, RAW):

                # assert that the file contents are in a valid valid format
                report_file = open(osr._report_file, 'r')
                report_data = report_file.readlines()[6:][0]
                report_file.close()
                try:
                    if osr_options['output_format'] == JSON:
                        json.loads(report_data)
                    else:
                        dict(eval(report_data))
                except ValueError as exc:
                    msg = str(exc).strip()
                    fmt = JSON.upper() if osr_options['output_format'] == JSON else 'Python dict'
                    self.fail('Data in {report_file} is not valid %{format} format. Exception: {error}'.format(report_file=osr._report_file, format=fmt, error=msg))

            else:
                # should never get here due to option assertions in gen_offload_status_report
                self.fail('output_format not found for {report_file}'.format(report_file=osr._report_file))

        Test.__init__(self, options, test_name, test_fn)


def create_sql_offload_tables(options, sh_test_api, table_name_re):
    """ Create table to be used in sql-offload test set.
    """
    c = get_test_cursor(options)
    try:
        # create offload test tables
        for table_name, script_path in sql_offload_test_table_scripts(options):
            if not table_name_re.search(table_name):
                continue

            try:
                sh_test_api.execute_ddl(f'DROP TABLE {table_name}', log_level=VERBOSE)
            except Exception as exc:
                log('Drop table exception: {}'.format(str(exc)), detail=verbose)

            sh_test_api.run_sql_in_file(script_path)

    finally:
        c.close()
        c.connection.close()


def sql_paths(directory, prefix=''):
    paths = glob.glob('%s*.sql' % os.path.join(directory, prefix))
    paths.sort()
    return paths


def log_session_id(ora_conn):
    if options.source_db_type == 'oracle':
        ora_curs = ora_conn.cursor()
        session_id = ora_curs.execute("SELECT USERENV('SID') FROM dual").fetchone()[0]
        log('Oracle session id: ' + str(session_id), verbose)


def log_transient_error_rerun_count():
    matches = get_lines_from_log(TRANSIENT_QUERY_RERUN_MARKER)
    retry_count = len(matches)
    if retry_count:
        log(f'Transient error retry count: {retry_count}')


def path_test_name(sql_path, parallel, query_rewrite=None, test_mode=None):
    test_name = '%s' % os.path.splitext(os.path.basename(sql_path))[0]
    if query_rewrite:
        test_name += '_qr_%s' % query_rewrite
    if parallel:
        test_name += '_pq'
    if test_mode:
        test_name += '_%s' % test_mode
    return test_name


def set_sql_test_modes(requested, valid):
    if requested is not None:
        return set.intersection(*[set(requested), set(valid)])
    else:
        return valid


def run_type_mapping_orchestration_tests(options, current_schema, orchestration_config, messages, should_run_test_f):
    tc_name = test_constants.SET_TYPE_MAPPING
    test_teamcity_starttestsuite(options, tc_name)

    try:
        frontend_api = get_frontend_testing_api(options, trace_action='run_type_mapping_orchestration_tests')
        backend_api = get_backend_testing_api(options)
        sh_test_api = frontend_api.create_new_connection(options.test_user, options.test_pass)

        run_type_mapping_offload_test(sh_test_api, backend_api, options, current_schema,
                                      orchestration_config, messages, Test, should_run_test_f)

        run_type_mapping_offload_column_tests(frontend_api, backend_api, options, current_schema,
                                              orchestration_config, messages, Test, should_run_test_f)
    except Exception as exc:
        log('Unhandled exception in %s test set:\n%s' % (tc_name, traceback.format_exc()))
        test_teamcity_failtest(options, tc_name, 'Fail: %s' % str(exc))
    finally:
        test_teamcity_endtestsuite(options, tc_name)


def generate_table(options, table_spec, ctas_row_limit=None):
    if len(table_spec) > 2:
        create_table(*table_spec[:5])
        if len(table_spec) >= 4:
            gather_table_stats(options, table_spec[0])
    elif len(table_spec) == 2:
        create_select(*table_spec + [ctas_row_limit])


def setup_test_tables(options, backend_api, sh_test_api, orchestration_config, messages, generated_view_config=[]):
    current_schema = get_schema()
    log('Schema: %s' % current_schema)

    max_decimal_precision = backend_api.max_decimal_precision()
    max_decimal_scale = backend_api.max_decimal_scale()
    max_decimal_integral_magnitude = backend_api.max_decimal_integral_magnitude()

    table_name_re = re.compile(options.filter, flags=re.I)

    create_sql_offload_tables(options, sh_test_api, table_name_re)

    # create programmatic data test tables
    for table_spec in generated_tables:
        table_spec[0] = '%s.%s' % (current_schema, table_spec[0])

        if not table_name_re.search(table_spec[0]):
            continue

        generate_table(options, table_spec)

    # Create views before offloading in order to have dependent views copied to hybrid schema
    c = get_test_cursor(options)
    try:
        for view_name, table_name, pred_list in generated_view_config:
            if not table_name_re.search(view_name):
                continue
            view_preds = ('WHERE '+' AND '.join(pred_list)) if pred_list else ''
            ddl = 'CREATE OR REPLACE FORCE VIEW %s.%s AS SELECT * FROM %s.%s %s' % \
                    (current_schema, view_name, current_schema, table_name, view_preds)
            log(ddl, detail=verbose)
            c.execute(ddl)
    finally:
        c.close()
        c.connection.close()

    # Generating test data sets a specific state in order to get the same data on all VMs.
    # Here we re-seed the generator to restore randomness.
    random.seed()

    # udf_data_db also used in offload section
    udf_data_db = get_data_db_for_schema(current_schema, orchestration_config)

    create_partition_functions(backend_api, udf_data_db, messages, table_name_re)

    # Offload programmatic data test tables
    for table_spec in generated_tables:
        table_name = table_spec[0].split('.')[1]

        if (table_name in [test_constants.PART_RANGE_TABLE_UNUM, test_constants.PART_RANGE_TABLE_UDEC,
                           test_constants.PART_RANGE_TABLE_UDEC2, test_constants.PART_RANGE_TABLE_USTR]
                and not backend_api.gluent_partition_functions_supported()):
            continue

        if not table_name_re.search(table_spec[0]):
            continue

        log('Offloading "%s"' % table_spec[0])
        config_overrides = {
            'execute': True,
            'log_path': options.log_path,
            'verbose': options.verbose,
            'vverbose': options.vverbose
        }
        offload_params = {
            'owner_table': table_spec[0],
            'reset_backend_table': True,
            'allow_floating_point_conversions': True,
            'allow_nanosecond_timestamp_columns': True,
            'max_offload_chunk_size': '0.5G',
            'synthetic_partition_digits': setup_constants.OFFLOAD_CUSTOM_DIGITS.get(table_name, '15'),
        }

        if len(table_spec) > 5:
            # We have a value for a 90/10 offload
            if isinstance(table_spec[5], tuple):
                format_fn = lambda x: x.strftime('%Y-%m-%d') if isinstance(x, datetime) else str(x)
                offload_params['equal_to_values'] = [format_fn(_) for _ in table_spec[5]]
            else:
                offload_params['less_than_value'] = table_spec[5]

        (offload_params['offload_partition_columns'], offload_params['offload_partition_granularity'],
         offload_params['offload_partition_lower_value'], offload_params['offload_partition_upper_value']) = \
            get_suitable_granularity_options(current_schema, table_name, orchestration_config)

        if table_name in [PART_LIST_TABLE_NUM, PART_LIST_AS_RANGE_TABLE_NUM,
                          PART_RANGE_TABLE_NUM, PART_RANGE_TABLE_DEC, PART_RANGE_TABLE_DEC2,
                          test_constants.PART_RANGE_TABLE_UNUM, test_constants.PART_RANGE_TABLE_UDEC,
                          test_constants.PART_RANGE_TABLE_UDEC2, test_constants.PART_RANGE_TABLE_USTR]:
            offload_params['offload_partition_granularity'] = str(1)
            offload_params['offload_partition_lower_value'] = str(PART_RANGE_NUM_VALUES[0])
            offload_params['offload_partition_upper_value'] = str(PART_RANGE_NUM_VALUES[-1])
            if table_name in [PART_RANGE_TABLE_DEC, PART_RANGE_TABLE_DEC2]:
                offload_params['decimal_columns_csv_list'] = ['column_1']
                if table_name == PART_RANGE_TABLE_DEC:
                    offload_params['decimal_columns_type_list'] = ['10,1']
                else:
                    offload_params['decimal_columns_type_list'] = ['30,10']
        if table_name in [test_constants.PART_RANGE_TABLE_BIG_INTS1, test_constants.PART_RANGE_TABLE_BIG_INTS2]:
            bigint_test_values = partition_key_test_numbers(16, 18)
            offload_params['integer_8_columns_csv'] = 'column_1'
            offload_params['offload_partition_lower_value'] = str(bigint_test_values[0])
            offload_params['offload_partition_upper_value'] = str(bigint_test_values[-1])
        if table_name in setup_constants.PART_RANGE_GRANULARITIES:
            offload_params['offload_partition_granularity'] = setup_constants.PART_RANGE_GRANULARITIES[table_name]
        if table_name in setup_constants.PART_RANGE_LOWER_VALUES:
            offload_params['offload_partition_lower_value'] = setup_constants.PART_RANGE_LOWER_VALUES[table_name]
        if table_name in setup_constants.PART_RANGE_UPPER_VALUES:
            offload_params['offload_partition_upper_value'] = setup_constants.PART_RANGE_UPPER_VALUES[table_name]
        if table_name == PART_RANGE_TABLE_DATE_DATE:
            offload_params['date_columns_csv'] = 'column_1'
        if table_name in setup_constants.PART_RANGE_PART_FUNCTIONS:
            offload_params['offload_partition_functions'] = f'{udf_data_db}.{setup_constants.PART_RANGE_PART_FUNCTIONS[table_name]}'
        if table_name == test_constants.GL_TYPE_MAPPING:
            # This table has a specific test set for offloading but we still need to get it offloaded in setup
            # in case tests don't run in ideal sequence. The actual test set will still re-offload and verify outcome
            offload_params.update(
                frontend_api.gl_type_mapping_offload_options(
                    max_decimal_precision, max_decimal_scale, max_decimal_integral_magnitude)
            )
        if orchestration_config.target == DBTYPE_HIVE:
            # Attempt to avoid ERROR_STATE in Hive, this setup is a once a day task and not part of test cycles
            offload_params['skip'] = step_title(command_steps.STEP_VALIDATE_DATA) + ',' + step_title(command_steps.STEP_VALIDATE_CASTS)

        OrchestrationRunner(config_overrides=config_overrides).offload(offload_params)

        # Create a set of functions for GL_TYPES...
        if table_name.lower() == 'gl_types':
            create_functions(options, table_spec[0], table_spec[2])

    normalise_options(options, normalise_owner_table=False)


def summarize_results():
    """ Print 'summary' of test results """

    if not g_test_results:
        dev_logger.warning("Uninitialized TestResults() object. Skipping summary")
        return

    g_test_results.aggregate()
    results = g_test_results.report_summary()
    # Breakdown by 'root cause' categories for 'failures' and 'errors'
    if results:
        results += g_test_results.report_details(states=[STATE_FAIL, STATE_ERROR], report_level=1)
        print(results)


def get_test_db_charset(orchestration_config):
    """ wrapper for gluent.get_db_charset because normalise_options is scattered to the wind and
        it's too risky to unpick where and when it is safe to do things
    """
    if orchestration_config.db_type == DBTYPE_ORACLE:
        return get_db_charset(orchestration_config)
    return None


def num_range_part(col_name, max_value, granularity):
    parts = ', '.join(['partition p_{0} values less than ({1}) storage (initial 64k)'.format(i, i + 1001)
                       for i in range(0, max_value, granularity)])
    return 'partition by range ({0}) ({1})'.format(col_name, parts)


def rdbms_part_clause_for_literals(col_name, literals, part_type='RANGE',
                                   simple_partition_names=False, add_maxvalue_partition=False):
    """ Generate an Oracle RANGE partition clause based on a list of literals it should contain.
    """
    def format_hwm_literal(val):
        if isinstance(val, (int, decimal.Decimal)):
            # RANGE requires less-than so add 1 to the value
            return str(val + 1) if part_type == 'RANGE' else str(val)
        elif isinstance(val, datetime):
            if part_type == 'RANGE':
                # RANGE requires less-than so add 1 day to the value
                return "DATE' %s'" % ((val + timedelta(days=1)).strftime('%Y-%m-%d'))
            else:
                return "DATE' %s'" % val.strftime('%Y-%m-%d')
        elif isinstance(val, str):
            # RANGE requires less-than but can't increment a string, just have to rely on caller requesting MAXVALUE
            return "'%s'" % val

    def format_part_name(val, part_num):
        if simple_partition_names:
            return f'P{part_num}'
        elif isinstance(val, int):
            return 'P%s' % str(val).replace('-', 'm')
        elif isinstance(val, datetime):
            return 'P%s' % val.strftime('%Y%m%d')
        elif isinstance(val, str):
            return 'P%s' % val[:5].replace('-', '_')

    if part_type == 'RANGE':
        maxval_partition = []
        if add_maxvalue_partition:
            maxval_partition = ['\nPARTITION PMV VALUES LESS THAN (MAXVALUE) STORAGE (INITIAL 64k)']
        parts = ', '.join(['\nPARTITION {0} VALUES LESS THAN ({1}) STORAGE (INITIAL 64k)'.format(format_part_name(val, i),
                                                                                                 format_hwm_literal(val))
                           for i, val in enumerate(literals)] + maxval_partition)
    else:
        parts = ', '.join(['\nPARTITION {0} VALUES ({1}) STORAGE (INITIAL 64k)'.format(format_part_name(val, i),
                                                                                       format_hwm_literal(val))
                           for i, val in enumerate(literals)])
    return '\nPARTITION BY {0} ({1}) ({2})'.format(part_type, col_name, parts)


def run_test_set(should_run_test, current_schema, orchestration_config, messages, test_set=None):
    """ Run a given test set or run all test sets if test_set is None
        Note the occasional set is not included unless explicitly named, e.g. datetime-full
    """
    if test_set is None or test_set == test_constants.SET_DIAGNOSE:
        run_diagnose_tests(options, should_run_test, current_schema, orchestration_config)

    if test_set is None or test_set == test_constants.SET_MSSQL_STORIES:
        run_story_tests(options, should_run_test, teamcity_name=test_constants.SET_MSSQL_STORIES)

    if test_set == test_constants.SET_OFFLOAD_TRANSPORT:
        run_offload_transport_tests(options, current_schema, orchestration_config, Test)

    if test_set is None or test_set == test_constants.SET_PRE_SALES:
        run_pre_sales_tests(options, should_run_test, current_schema)

    if test_set is None or test_set == test_constants.SET_PYTHON_UNIT:
        run_python_unit_tests(options, orchestration_config, messages, should_run_test)

    if test_set is None or test_set == test_constants.SET_SQL_OFFLOAD:
        run_sql_offload_tests(options, current_schema, orchestration_config, Test)

    if test_set is None or test_set == test_constants.SET_OSR:
        run_offload_status_report_tests(options, should_run_test, current_schema)

    if test_set == test_constants.SET_STORIES_CONTINUOUS:
        run_story_tests(options, should_run_test, teamcity_name=test_constants.SET_STORIES_CONTINUOUS)

    if test_set in (test_constants.SET_STORIES_INTEGRATION, test_constants.SET_STORIES):
        run_story_tests(options, should_run_test, teamcity_name=test_constants.SET_STORIES_INTEGRATION)

    if test_set is None or test_set == test_constants.SET_TYPE_MAPPING:
        run_type_mapping_orchestration_tests(options, current_schema, orchestration_config, messages, should_run_test)

    log_transient_error_rerun_count()


def generated_tables_entry(table_name, num_rows, column_spec, partition_clause=None,
                           column_names=None, offload_ipa_hwm=None):
    """ Helper function to help me (NJ) remember what the fields are in
        generated_tables list of lists with nested lists!
    """
    assert table_name
    assert num_rows
    assert column_spec
    return_list = [table_name, num_rows, column_spec]
    if partition_clause:
        return_list.extend([partition_clause, column_names])
        if offload_ipa_hwm:
            return_list.append(offload_ipa_hwm)
    return return_list


def include_gl_types_id_column(orchestration_config, frontend_api) -> bool:
    return bool(orchestration_config.db_type != DBTYPE_ORACLE or
                LooseVersion(frontend_api.frontend_version()) >= LooseVersion('11.1.0'))


def should_run_test(test_name):
    """ Wrapper over function shared by 'test' and 'test_runner' """
    return test_passes_filter(test_name, test_name_re, options, known_failure_blacklist)


def get_default_tests_script_path(directory_name):
    return '../sql/oracle/tests/%s' % directory_name


if __name__ == '__main__':
    opt = get_options()
    add_common_test_options(opt)
    opt.add_option('--setup', dest='setup', action='store_true', default=False)
    opt.add_option('--no-newlines', dest='no_newlines', action='store_true', default=False)
    add_test_set_options(opt)
    add_common_test_run_options(opt)
    opt.add_option('--set-benchmark', dest='set_benchmark', default=False, action='store_true')
    opt.add_option('--customer', dest='customer', default=None,
                   help='Use in combination with --set=sql-customers. Customer subdirectories in %s by default'
                   % get_test_set_sql_path('sql_customers'))
    opt.add_option('--stress-factor', dest='stress_factor', default=4,
                   help='Number of concurrent threads to keep filled with query activity')
    opt.add_option('--stress-duration-mins', dest='stress_duration_mins', default=2)
    opt.add_option('--smoke-size', dest='smoke_size', default=10, help='Number of tests per set to run in the smoke test')
    opt.add_option('--unique-hint', dest='unique_hint', default=True)
    opt.add_option('--test-adm-user', dest='ora_test_adm_user', default='system',
                   help='Name of an Oracle DBA user for use in test setup')
    opt.add_option('--test-adm-pass', dest='ora_test_adm_pass', default='oracle',
                   help='Password for an Oracle DBA user for use in test setup')
    opt.add_option('--sql-test-mode', dest='sql_test_mode', help='run|parse|data|rewrite. Filter for SQL tests only (see --sql-test-dir). Run executes the query to completion, parse parses the query to check for validity, data builds and executes a query to check for data correctness, rewrite checks that query rewrite happens when expected.')
    opt.add_option('--summary', dest='summary', default=False, action='store_true', help='Print test run summary')
    opt.add_option('--root-cause-config', dest='root_cause_config', default='test-root-causes.yaml', help='(YAML) configuration file with "general" test root causes')
    opt.add_option('--cloud-sync-test-file', dest='cloud_sync_test_file', default='../config/tests/offload/cloud_sync_tests.yaml', help='(YAML) File with cloud_sync tests (Format: name: shell test command)')
    opt.add_option('--cloud-sync-remote-offload-config', dest='cloud_sync_remote_offload_config', default='../config/tests/offload/remote-offload.conf', help='(YAML) File with cloud_sync tests (Format: name: shell test command)')
    opt.add_option('--continuous', dest='continuous', action='store_true', default=False, help='Run a lighter set of tests for continuous testing')
    opt.add_option('--keyfile', dest='password_key_file', default=orchestration_defaults.password_key_file_default(),
                   help=SUPPRESS_HELP)
    add_story_test_options(opt)
    opt = get_oracle_options(opt)

    options, args = opt.parse_args()

    init(options)
    init_log('test')

    options.stress_factor = int(options.stress_factor)
    options.stress_duration_mins = int(options.stress_duration_mins)
    default_parallel = int(options.test_pq_degree) if options.test_pq_degree else min([multiprocessing.cpu_count(), 4])

    options.smoke_size = int(options.smoke_size)

    benchmarks = Benchmarks()
    benchmarks.load()

    # Initialize TestResults
    if options.summary:
        try:
            g_test_results = TestResults(options.root_cause_config, orchestration_defaults.query_engine_default())
        except Exception as e:
            dev_logger.warn("Exception detected while initializing TestResults object: %s. Turning 'summary' off" % e)
            options.summary = False

    options.execute = True
    options.verify_row_count = False
    options.enable_prompts = False

    options.db_type = orchestration_defaults.frontend_db_type_default()
    options.source_db_type = options.db_type
    normalise_rdbms_options(options)
    normalise_test_pass_options(options)

    orchestration_config = OrchestrationConfig.from_dict({'execute': options.execute,
                                                          'verbose': options.verbose,
                                                          'vverbose': options.vverbose},
                                                         do_not_connect=options.list_stories)
    backend_api = get_backend_testing_api(options)
    frontend_api = get_frontend_testing_api(options, trace_action='FrontendTestingApi(Test)')
    messages = OffloadMessages.from_options(options, get_log_fh())

    # gl_dba_objects_tab_v has a predicate inside the view
    # If we add views on objects other than gl_dba_objects we need to modify primitive check in run_predicate_tests()
    generated_view_config = [['gl_dba_objects_v', 'gl_dba_objects', None],
                             ['gl_dba_objects_tab_v', 'gl_dba_objects', ['object_type = \'TABLE\'']]]
    gl_type_mapping_specs, gl_type_mapping_names = backend_api.gl_type_mapping_generated_table_col_specs()
    generated_tables_backend = [[test_constants.GL_BACKEND_TYPE_MAPPING, 20,
                                 gl_type_mapping_specs, gl_type_mapping_names]]
    gl_identifiers_specs, gl_identifiers_names = backend_api.gl_identifiers_generated_table_col_specs()

    if options.setup:
        sh_test_api = frontend_api.create_new_connection(options.test_user, options.test_pass)
        os.environ['NLS_LANG'] = '.AL32UTF8'
        options.oracle_charset = database_charset(options)
        options.python_charset = oracle_to_py_charset(options.oracle_charset)
        code_point_to_char = code_point_fn(options.python_charset)

        def ascii7_nonull_if_synapse(else_constant=None):
            if else_constant == ordered:
                return ordered_ascii7_nonull if orchestration_config.target == DBTYPE_SYNAPSE else else_constant
            else:
                return ascii7_nonull if orchestration_config.target == DBTYPE_SYNAPSE else else_constant

        # generated_tables format #1 (this really needs improving, a dict would make more sense. nj@2020-08-24
        # [
        #   table_name,
        #   num_rows,
        #   [list, of, column, specs],
        #   "partition by" clause,
        #   [list, of, column, names],
        #   a high value to use when offloading, nothing to do with creating a table,
        # ]
        # generated_tables format #2
        # [
        #   table_name,
        #   table_name to CTAS from,
        # ]
        no_names = None
        generated_tables = []
        # Start the gl_wide table with a few fixed leading columns to enable the table to be used in static SQL tests...
        # Limiting columns in GL_WIDE due to Synapse error:
        #      [SQL Server]Cannot create a row of size 9436 which is greater than the allowable maximum row size of 8060
        gl_wide_extra_columns = gl_wide_max_columns(frontend_api, backend_api) - 20
        generated_tables += [['gl_wide', 100,
                              [ORACLE_TYPE_NUMBER for i in range(5)] +
                              [(ORACLE_TYPE_VARCHAR, 10, ascii7) for i in range(5)] +
                              [ORACLE_TYPE_DATE for i in range(5)] +
                              [ORACLE_TYPE_TIMESTAMP for i in range(5)] +
                              [choice([(ORACLE_TYPE_VARCHAR, 10, ascii7_nonull_if_synapse()),
                                       ORACLE_TYPE_NUMBER,
                                       ORACLE_TYPE_DATE,
                                       ORACLE_TYPE_TIMESTAMP])
                               for i in range(gl_wide_extra_columns)]]]
        generated_tables += [['gl_chars', 512, [(ORACLE_TYPE_CHAR, 100, ascii7_nonull_if_synapse(else_constant=ordered)),
                                                (ORACLE_TYPE_NCHAR, 100, ascii7_nonull_if_synapse(else_constant=ordered))]]]
        gl_types_char_columns = [(ORACLE_TYPE_CHAR, 100, ascii7_nonull_if_synapse(else_constant=ascii7)),
                                 (ORACLE_TYPE_VARCHAR, 1100, ascii7_nonull_if_synapse()),
                                 (ORACLE_TYPE_NCHAR, 100, ascii7_nonull_if_synapse()),
                                 (ORACLE_TYPE_NVARCHAR2, 1100, ascii7_nonull_if_synapse())]
        gl_types_date_columns = [ORACLE_TYPE_DATE,
                                 (ORACLE_TYPE_TIMESTAMP, 0),
                                 ORACLE_TYPE_TIMESTAMP,
                                 ORACLE_TYPE_TIMESTAMP_TZ]
        # GOE-1503: due to a cx_Oracle 7.1.0 - 7.3.0 issue with binding over 36 digits for negative numbers, we've
        # temporarily dropped the spec to 36. We'll raise a bug with cx_Oracle and re-instate extreme testing when
        # a fix is available.
        gl_types_big_integer_precision = 36  # GOE-1503: 38
        if backend_api.canonical_float_supported():
            float32_types = [ORACLE_TYPE_BINARY_FLOAT]
        else:
            float32_types = []
        gl_types_num_columns = [ORACLE_TYPE_NUMBER,
                                (ORACLE_TYPE_NUMBER, 2),
                                (ORACLE_TYPE_NUMBER, 4),
                                (ORACLE_TYPE_NUMBER, 9),
                                (ORACLE_TYPE_NUMBER, 18),
                                (ORACLE_TYPE_NUMBER, gl_types_big_integer_precision),
                                (ORACLE_TYPE_NUMBER, None, 0)] + float32_types + \
                               [ORACLE_TYPE_BINARY_DOUBLE,
                                ORACLE_TYPE_FLOAT]
        gl_types_interval_columns = ['INTERVAL YEAR(9) TO MONTH',
                                     'INTERVAL DAY(9) TO SECOND(9)']
        gl_types_lob_columns = [(ORACLE_TYPE_RAW, 16),
                                (ORACLE_TYPE_RAW, 2000),
                                (ORACLE_TYPE_CLOB, ascii7_nonull_if_synapse()),
                                (ORACLE_TYPE_BLOB, ascii7_nonull_if_synapse()),
                                (ORACLE_TYPE_NCLOB, ascii7_nonull_if_synapse())]

        generated_tables += [['gl_types', 10000, gl_types_char_columns \
                                               + gl_types_date_columns \
                                               + gl_types_interval_columns \
                                               + gl_types_num_columns \
                                               + gl_types_lob_columns]]

        if include_gl_types_id_column(orchestration_config, frontend_api):
            generated_tables_extra_cols['gl_types'] = ['id_group number(1) as (mod(id,10)) virtual']
        generated_tables += [['gl_types_part', 10000, gl_types_char_columns \
                                                    + gl_types_date_columns \
                                                    + gl_types_interval_columns \
                                                    + gl_types_num_columns,
                              num_range_part('ID', 10000, 1000)]]
        # gl_types variant that will allow query import to tests due to
        # omission of interval types and reduced size.
        generated_tables += [['gl_types_qi', 100, gl_types_char_columns \
                                                + gl_types_date_columns \
                                                + gl_types_num_columns \
                                                + gl_types_lob_columns]]
        generated_tables += [['gl_dba_objects', 'sys.dba_objects']]

        # Start the gl_fap% tables with a few fixed leading columns to enable the table to be used in static SQL tests...
        gl_fap_columns = [ORACLE_TYPE_NUMBER for i in range(2)] \
                       + [('VARCHAR', 10, ascii7) for i in range(2)] \
                       + ['DATE' for i in range(2)] \
                       + [choice([('VARCHAR', 30, ascii7), ORACLE_TYPE_NUMBER, 'DATE']) for i in range(194)]
        generated_tables += [['gl_fap', 10000, gl_fap_columns]]
        generated_tables += [['gl_fap_part', 10000, gl_fap_columns, num_range_part('ID', 10000, 1000), no_names, 9001]]

        # Test table with long column names
        gl_identifier_columns = [(ORACLE_TYPE_NUMBER, 9),
                                 (ORACLE_TYPE_NUMBER, 18),
                                 (ORACLE_TYPE_NUMBER, 28),
                                 ORACLE_TYPE_DATE,
                                 ORACLE_TYPE_TIMESTAMP,
                                 (ORACLE_TYPE_VARCHAR, 30, ascii7_nonull_if_synapse())]
        max_hybrid_identifier_length = frontend_api.max_table_name_length()
        def gl_identifier_name(x):
            return 'COL_{}_{}'.format(x[0], x[1]) if isinstance(x, tuple) else 'COL_{}'.format(x)
        gl_identifier_names = [gen_max_length_identifier(gl_identifier_name(_), max_hybrid_identifier_length)
                               for _ in gl_identifier_columns]
        generated_tables += [[GL_IDENTIFIERS_TABLE, 100, gl_identifier_columns, '', gl_identifier_names, None]]

        # Tables used to test hybrid queries on partition key columns.
        # The 4th item in the colspec list is a list of literals to use for partition key column.
        # The 5th item in each table spec is a high value for offload. A single value means RANGE
        # a tuple means LIST offload.
        generated_tables.append(
            generated_tables_entry(PART_RANGE_TABLE_NUM, 100,
                                   [(ORACLE_TYPE_NUMBER, 4, 0, PART_RANGE_NUM_VALUES),
                                    (ORACLE_TYPE_VARCHAR2, 10, ascii7)],
                                   partition_clause=rdbms_part_clause_for_literals('COLUMN_1', PART_RANGE_NUM_VALUES),
                                   offload_ipa_hwm=PART_RANGE_NUM_VALUES[-1])
        )
        generated_tables.append(
            generated_tables_entry(PART_RANGE_TABLE_DATE, 100,
                                   [(ORACLE_TYPE_DATE, None, PART_RANGE_DATE_VALUES),
                                    (ORACLE_TYPE_VARCHAR2, 10, ascii7)],
                                   partition_clause=rdbms_part_clause_for_literals('COLUMN_1', PART_RANGE_DATE_VALUES),
                                   offload_ipa_hwm=PART_RANGE_DATE_VALUES[-1])
        )
        if backend_api.canonical_date_supported():
            generated_tables += [[PART_RANGE_TABLE_DATE_DATE, 100,
                                  [(ORACLE_TYPE_DATE, None, PART_RANGE_DATE_VALUES),
                                   (ORACLE_TYPE_VARCHAR2, 10, ascii7)],
                                  rdbms_part_clause_for_literals('COLUMN_1', PART_RANGE_DATE_VALUES), no_names,
                                  PART_RANGE_DATE_VALUES[-1]]]
        generated_tables.append(
            generated_tables_entry(PART_LIST_TABLE_NUM, 100,
                                   [(ORACLE_TYPE_NUMBER, 4, 0, PART_RANGE_NUM_VALUES),
                                    (ORACLE_TYPE_VARCHAR2, 10, ascii7)],
                                   partition_clause=rdbms_part_clause_for_literals('COLUMN_1', PART_RANGE_NUM_VALUES,
                                                                                   part_type='LIST'),
                                   offload_ipa_hwm=tuple(PART_RANGE_NUM_VALUES[:-1]))
        )
        generated_tables.append(
            generated_tables_entry(PART_LIST_TABLE_DATE, 100,
                                   [(ORACLE_TYPE_DATE, None, PART_RANGE_DATE_VALUES),
                                    (ORACLE_TYPE_VARCHAR2, 10, ascii7)],
                                   partition_clause=rdbms_part_clause_for_literals('COLUMN_1', PART_RANGE_DATE_VALUES,
                                                                                   part_type='LIST'),
                                   offload_ipa_hwm=tuple(PART_RANGE_DATE_VALUES[:-1]))
        )
        generated_tables.append(
            generated_tables_entry(PART_LIST_AS_RANGE_TABLE_NUM, 100,
                                   [(ORACLE_TYPE_NUMBER, 4, 0, PART_RANGE_NUM_VALUES),
                                    (ORACLE_TYPE_VARCHAR2, 10, ascii7)],
                                   partition_clause=rdbms_part_clause_for_literals('COLUMN_1', PART_RANGE_NUM_VALUES,
                                                                                   part_type='LIST'),
                                   offload_ipa_hwm=PART_RANGE_NUM_VALUES[-1])
        )
        generated_tables.append(
            generated_tables_entry(PART_LIST_AS_RANGE_TABLE_DATE, 100,
                                   [(ORACLE_TYPE_DATE, None, PART_RANGE_DATE_VALUES),
                                    (ORACLE_TYPE_VARCHAR2, 10, ascii7)],
                                   partition_clause=rdbms_part_clause_for_literals('COLUMN_1', PART_RANGE_DATE_VALUES,
                                                                                   part_type='LIST'),
                                   offload_ipa_hwm=PART_RANGE_DATE_VALUES[-1])
        )
        generated_tables.append(
            generated_tables_entry(PART_RANGE_TABLE_DEC, 100,
                                   [(ORACLE_TYPE_NUMBER, 4, 0, PART_RANGE_NUM_VALUES),
                                    (ORACLE_TYPE_VARCHAR2, 10, ascii7)],
                                   partition_clause=rdbms_part_clause_for_literals('COLUMN_1', PART_RANGE_NUM_VALUES),
                                   offload_ipa_hwm=PART_RANGE_NUM_VALUES[-1])
        )
        generated_tables.append(
            generated_tables_entry(PART_RANGE_TABLE_DEC2, 100,
                                   [(ORACLE_TYPE_NUMBER, 4, 0, PART_RANGE_NUM_VALUES),
                                    (ORACLE_TYPE_VARCHAR2, 10, ascii7)],
                                   partition_clause=rdbms_part_clause_for_literals('COLUMN_1', PART_RANGE_NUM_VALUES),
                                   offload_ipa_hwm=PART_RANGE_NUM_VALUES[-1])
        )
        generated_tables.append(
            generated_tables_entry(PART_RANGE_TABLE_STR, 100,
                                   [(ORACLE_TYPE_VARCHAR2, 4, None, PART_RANGE_STR_VALUES),
                                    (ORACLE_TYPE_VARCHAR2, 10, ascii7),
                                    (ORACLE_TYPE_NUMBER, 10, 0)],
                                   partition_clause=rdbms_part_clause_for_literals('COLUMN_1', PART_RANGE_STR_HVS,
                                                                                   simple_partition_names=True),
                                   offload_ipa_hwm=PART_RANGE_STR_HVS[-1])
        )
        # Generated tables for partition function UDF offloads
        generated_tables.append(
            generated_tables_entry(test_constants.PART_RANGE_TABLE_UNUM, 100,
                                   [(ORACLE_TYPE_NUMBER, 4, 0, PART_RANGE_NUM_VALUES),
                                    (ORACLE_TYPE_VARCHAR2, 10, ascii7)],
                                   partition_clause=rdbms_part_clause_for_literals('COLUMN_1', PART_RANGE_NUM_VALUES),
                                   offload_ipa_hwm=PART_RANGE_NUM_VALUES[-1])
        )
        generated_tables.append(
            generated_tables_entry(test_constants.PART_RANGE_TABLE_UDEC, 100,
                                   [(ORACLE_TYPE_NUMBER, 38, 0, PART_RANGE_NUM_VALUES),
                                    (ORACLE_TYPE_VARCHAR2, 10, ascii7)],
                                   partition_clause=rdbms_part_clause_for_literals('COLUMN_1', PART_RANGE_NUM_VALUES),
                                   offload_ipa_hwm=PART_RANGE_NUM_VALUES[-1])
        )
        generated_tables.append(
            generated_tables_entry(test_constants.PART_RANGE_TABLE_UDEC2, 100,
                                   [(ORACLE_TYPE_NUMBER, 19, 0, PART_RANGE_NUM_VALUES),
                                    (ORACLE_TYPE_VARCHAR2, 10, ascii7)],
                                   partition_clause=rdbms_part_clause_for_literals('COLUMN_1', PART_RANGE_NUM_VALUES),
                                   offload_ipa_hwm=PART_RANGE_NUM_VALUES[-1])
        )
        generated_tables.append(
            generated_tables_entry(test_constants.PART_RANGE_TABLE_USTR, 100,
                                   [(ORACLE_TYPE_VARCHAR2, 4, None, PART_RANGE_STR_VALUES),
                                    (ORACLE_TYPE_VARCHAR2, 10, ascii7),
                                    (ORACLE_TYPE_NUMBER, 10, 0)],
                                   partition_clause=rdbms_part_clause_for_literals('COLUMN_1', PART_RANGE_STR_HVS,
                                                                                   simple_partition_names=True),
                                   offload_ipa_hwm=PART_RANGE_STR_HVS[-1])
        )
        # Generated tables for testing predicates on extreme partition keys
        bigint_test_values = partition_key_test_numbers(16, 18)
        generated_tables.append(
            generated_tables_entry(test_constants.PART_RANGE_TABLE_BIG_INTS1, 100,
                                   [(ORACLE_TYPE_NUMBER, 19, 0, bigint_test_values),
                                    (ORACLE_TYPE_VARCHAR2, 10, ascii7)],
                                   partition_clause=rdbms_part_clause_for_literals('COLUMN_1', bigint_test_values[-1:],
                                                                                   simple_partition_names=True),
                                   )
        )
        bigint_test_values = partition_key_test_numbers(16, 18)
        generated_tables.append(
            generated_tables_entry(test_constants.PART_RANGE_TABLE_BIG_INTS2, 100,
                                   [(ORACLE_TYPE_NUMBER, 19, 0, bigint_test_values),
                                    (ORACLE_TYPE_VARCHAR2, 10, ascii7)],
                                   partition_clause=rdbms_part_clause_for_literals('COLUMN_1', bigint_test_values[-1:],
                                                                                   simple_partition_names=True),
                                   )
        )
        if orchestration_config.target == DBTYPE_BIGQUERY:
            # Partitioning with GDP on BigQuery is currently limited to INT64
            decint1_test_values = partition_key_test_numbers(16, 18)
        else:
            decint1_test_values = partition_key_test_numbers(16, 19)
        generated_tables.append(
            generated_tables_entry(test_constants.PART_RANGE_TABLE_DEC_INTS1, 100,
                                   [(ORACLE_TYPE_NUMBER, 20, 0, decint1_test_values),
                                    (ORACLE_TYPE_VARCHAR2, 10, ascii7)],
                                   partition_clause=rdbms_part_clause_for_literals('COLUMN_1',
                                                                                   decint1_test_values[-1:],
                                                                                   simple_partition_names=True),
                                   )
        )
        generated_tables.append(
            generated_tables_entry(test_constants.PART_RANGE_TABLE_DEC_INTS2, 100,
                                   [(ORACLE_TYPE_NUMBER, 20, 0, decint1_test_values),
                                    (ORACLE_TYPE_VARCHAR2, 10, ascii7)],
                                   partition_clause=rdbms_part_clause_for_literals('COLUMN_1',
                                                                                   decint1_test_values[-1:],
                                                                                   simple_partition_names=True),
                                   )
        )
        if orchestration_config.target in [DBTYPE_IMPALA, DBTYPE_HIVE]:
            # Partitioning with GDP on BigQuery is currently limited to INT64 so these tests only make sense on Hadoop
            dec_max_precision = min(backend_api.max_decimal_precision(), 38)
            decint3_test_values = partition_key_test_numbers(min(dec_max_precision, 36), dec_max_precision)
            # Use penultimate test value for HV assuming the last one is the max for Oracle and will end up in MAXVALUE
            generated_tables.append(
                generated_tables_entry(test_constants.PART_RANGE_TABLE_DEC_INTS3, 100,
                                       [(ORACLE_TYPE_NUMBER, 38, 0, decint3_test_values),
                                        (ORACLE_TYPE_VARCHAR2, 10, ascii7)],
                                       partition_clause=rdbms_part_clause_for_literals('COLUMN_1',
                                                                                       decint3_test_values[-2:-1],
                                                                                       simple_partition_names=True,
                                                                                       add_maxvalue_partition=True),
                                       )
            )
            generated_tables.append(
                generated_tables_entry(test_constants.PART_RANGE_TABLE_DEC_INTS4, 100,
                                       [(ORACLE_TYPE_NUMBER, 38, 0, decint3_test_values),
                                        (ORACLE_TYPE_VARCHAR2, 10, ascii7)],
                                       partition_clause=rdbms_part_clause_for_literals('COLUMN_1',
                                                                                       decint3_test_values[-2:-1],
                                                                                       simple_partition_names=True,
                                                                                       add_maxvalue_partition=True),
                                       )
            )
        if orchestration_config.target == DBTYPE_BIGQUERY:
            # Partitioning with GDP on BigQuery is currently limited to INT64
            big_dec_test_values = [decimal.Decimal(('9' * 15) + str(TestDecimal.rnd(11,9))) for _ in range(100)]
            big_dec_final_hv = decimal.Decimal('9' * 18)
        else:
            big_dec_test_values = [decimal.Decimal(('9' * 20) + str(TestDecimal.rnd(11, 9))) for _ in range(100)]
            big_dec_final_hv = decimal.Decimal('9' * 25)
        generated_tables.append(
            generated_tables_entry(test_constants.PART_RANGE_TABLE_BIG_DECS1, 20,
                                   [(ORACLE_TYPE_NUMBER, 38, 9, big_dec_test_values),
                                    (ORACLE_TYPE_VARCHAR2, 10, ascii7)],
                                   partition_clause=rdbms_part_clause_for_literals('COLUMN_1', [big_dec_final_hv],
                                                                                   simple_partition_names=True)
                                   )
        )
        generated_tables.append(
            generated_tables_entry(test_constants.PART_RANGE_TABLE_BIG_DECS2, 20,
                                   [(ORACLE_TYPE_NUMBER, 38, 9, big_dec_test_values),
                                    (ORACLE_TYPE_VARCHAR2, 10, ascii7)],
                                   partition_clause=rdbms_part_clause_for_literals('COLUMN_1', [big_dec_final_hv],
                                                                                   simple_partition_names=True)
                                   )
        )

        gl_type_mapping_specs, gl_type_mapping_names =\
            frontend_api.gl_type_mapping_generated_table_col_specs(
                backend_api.max_decimal_precision(),
                backend_api.max_decimal_scale(),
                backend_api.max_decimal_integral_magnitude(),
                list(backend_api.expected_canonical_to_backend_type_map().keys()),
                ascii_only=bool(orchestration_config.target == DBTYPE_SYNAPSE))
        if not backend_api.canonical_float_supported():
            list_items_to_remove = [i for i, j in enumerate(gl_type_mapping_specs)
                                    if j == ORACLE_TYPE_BINARY_FLOAT or (isinstance(j, tuple)
                                                                         and j[0] == ORACLE_TYPE_BINARY_FLOAT)]
            for i in list_items_to_remove:
                del gl_type_mapping_specs[i]
                del gl_type_mapping_names[i]
        generated_tables += [[test_constants.GL_TYPE_MAPPING, 100, gl_type_mapping_specs, '', gl_type_mapping_names]]

        setup_test_tables(options, backend_api, sh_test_api, orchestration_config, messages,
                          generated_view_config=generated_view_config)
    elif options.list_stories:
        story_runner.list_stories(options, messages, orchestration_config)
    else:
        current_schema = get_schema()
        hybrid_schema = get_hybrid_schema(current_schema)

        for tn in ['gl_wide', 'gl_chars', 'gl_types', 'gl_dba_objects',
                   'gl_types_qi', test_constants.GL_TYPE_MAPPING, GL_IDENTIFIERS_TABLE]:
            owner_table = '%s.%s' % (current_schema, tn)
            generated_tables.append([tn, owner_table])
        generated_views = [[vn, '%s.%s' % (current_schema, vn)] for (vn, tn, _) in generated_view_config]
        # collapse generated_tables_backend to match lists above, also use hybrid schema because there's no RDBMS table
        generated_tables_backend = [[tn, '%s_H.%s' % (current_schema, tn)]
                                    for tn in [_[0] for _ in generated_tables_backend]]

        if options.filter:
            log('Test filter: %s' % options.filter)

        test_name_re = re.compile(options.filter, flags=re.I)

        known_failure_blacklist = gen_known_failure_blacklist(orchestration_config, frontend_api, backend_api)

        if get_test_db_charset(orchestration_config) == 'WE8ISO8859P15':
            known_failure_blacklist += SINGLE_BYTE_CHARSET_KNOWN_FAILURE_BLACKLIST

        if "00:00" not in get_tz():
            known_failure_blacklist += GOE_1764_KNOWN_FAILURE_BLACKLIST
            known_failure_blacklist += GOE_1546_KNOWN_FAILURE_BLACKLIST

        options.known_failure_blacklist = known_failure_blacklist

        if options.sql_test_mode is not None:
            options.sql_test_mode = options.sql_test_mode.split(',')

        messages = OffloadMessages.from_options(options, get_log_fh())
        if options.test_set is not None:
            options.test_set = options.test_set.split(',')
            # Running each named set in the order they were specified
            for test_set in options.test_set:
                run_test_set(should_run_test, current_schema, orchestration_config, messages, test_set)
        else:
            # Default run of most sets in the order they are listed inside run_test_set()
            run_test_set(should_run_test, current_schema, orchestration_config, messages)

    if options.set_benchmark:
        benchmarks.set_benchmark()

    if options.summary:
        summarize_results()

    benchmarks.save()

    log(END_OF_TEST_RUN_MARKER)
