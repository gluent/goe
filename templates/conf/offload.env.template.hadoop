# Copyright 2016 The GOE Authors. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# ===========================================================================================
# Hadoop settings
# ===========================================================================================

# Hadoop/FS settings:
export HADOOP_SSH_USER=${OFFLOAD_TRANSPORT_USER}
export HDFS_HOME=/user/goe
# The HDFS location for both temporary load and final offloaded tables. OFFLOAD_FS_PREFIX, when set, takes
# precedence over HDFS_DATA for Offloaded tables
export HDFS_DATA=${HDFS_HOME}/offload
export HDFS_LOAD=${HDFS_DATA}

# Filesystem type for Offloaded tables. Valid values are:
#   inherit: Do not include a LOCATION clause when creating a table, inherit the value from the container database
#   hdfs:    Use a LOCATION clause to store table data in HDFS in the local cluster
#   s3a:     Use a LOCATION clause to store table data in Amazon S3. This must be correctly configured in the backend system configuration
#   adl:     Use a LOCATION clause to store table data in Microsoft Azure Data Lake Storage Generation 1. This must be correctly configured in the backend system configuration
#   abfs(s): Use a LOCATION clause to store table data in Microsoft Azure Data Lake Storage Generation 2. This must be correctly configured in the backend system configuration
# When offloading a table to cloud storage the table LOCATION will be structured as below:
#   ${OFFLOAD_FS_SCHEME}://${OFFLOAD_FS_CONTAINER}/${OFFLOAD_FS_PREFIX}/db_name/table_name/
export OFFLOAD_FS_SCHEME=inherit
# The path with which to prefix offloaded table paths. Takes precedence over HDFS_DATA when OFFLOAD_FS_SCHEME != "inherit"
export OFFLOAD_FS_PREFIX=${HDFS_DATA}
# A valid bucket or container name when offloading to cloud storage
export OFFLOAD_FS_CONTAINER=

# HDFS client configuration file location
#export LIBHDFS3_CONF=${OFFLOAD_HOME}/conf/hdfs-client.xml

# WEBHDFS_HOST/PORT can be used to optimize HDFS activities removing JVM start-up overhead by utilising WebHDFS
# WEBHDFS_HOST can be a comma-separated list of hosts if HDFS is configured for High Availability
# If WEBHDFS_PORT is unset then default ports of 50070 (HTTP) or 50470 (HTTPS) are used
# WEBHDFS_VERIFY_SSL is used to enable SSL for WebHDFS calls. There are 4 states:
#   Empty: Do not use SSL
#   TRUE: Use SSL & verify Hadoop certificate against known certificates
#   FALSE: Use SSL & do not verify Hadoop certificate
#   /some/path/here/cert-bundle.crt: Use SSL & verify Hadoop certificate against path to certificate bundle
#export WEBHDFS_HOST=
#export WEBHDFS_PORT=
export WEBHDFS_VERIFY_SSL=

# Impala/Hive connection settings:
# (HIVE_SERVER_HOST/PORT are used for both Impala and Hive connections)
# HIVE_SERVER_HOST can be a comma-separated list of hosts to randomly choose from, e.g.: hdp21,hdp22,hdp23
# Default Impala port is 21050, default Hive port is 10000
export HIVE_SERVER_HOST=<hostname>
export HIVE_SERVER_PORT=21050
export HIVE_SERVER_USER=goe
export HIVE_SERVER_PASS=
# Use HTTP transport for HiveServer2 connections
#export HIVE_SERVER_HTTP_TRANSPORT=true
# Path component of URL endpoint when connecting to HiveServer2 in HTTP mode
#export HIVE_SERVER_HTTP_PATH=
# Path to LDAP password file
#export HIVE_SERVER_LDAP_PASSWORD_FILE=
# Hive connection timeout in seconds
export HIVE_SERVER_TIMEOUT=3600

# Authentication mechanism for HiveServer2
# In non-kerberized environments, should be set to:
#   impala: NOSASL
#   hive:   value of hive-site.xml: hive.server2.authentication
# Ignored in kerberized or LDAP environments
#export HIVE_SERVER_AUTH_MECHANISM=NOSASL

# Kerberos settings:
# KERBEROS_SERVICE is the SQL engine Kerberos service (usually 'impala' or 'hive')
# KERBEROS_KEYTAB is the path of the keytab file
#  if not provided, a valid ticket must already exist in the cache (i.e. manual kinit)
# if KERBEROS_KEYTAB is provided, KERBEROS_PRINCIPAL should also be provided
#  it is the kerberos user to authenticate as. ie $kinit -kt KERBEROS_KEYTAB KERBEROS_PRINCIPAL should succeed
# Set KERBEROS_PATH if your Kerberos utilities (like kinit) reside in some non-standard directory
export KERBEROS_SERVICE=
export KERBEROS_KEYTAB=
export KERBEROS_PRINCIPAL=
export KERBEROS_PATH=/usr/kerberos/bin
# KERBEROS_TICKET_CACHE_PATH is required to use the libhdfs3-based HDFS result cache in a kerberized cluster
# For example: /tmp/krb5cc_12345
export KERBEROS_TICKET_CACHE_PATH=

# Integrate with a data governance backend. If the URL is blank then integration is disabled. Format:
#      http://fqdn-n.example.com:port/api
#export DATA_GOVERNANCE_API_URL=
#export DATA_GOVERNANCE_API_USER=
#export DATA_GOVERNANCE_API_PASS=
export DATA_GOVERNANCE_BACKEND=navigator
# CLOUDERA_NAVIGATOR_HIVE_SOURCE_ID is mandatory when integrating with Cloudera Navigator
# This can be determined by taking the "identity" value from the output of a curl command like the example below:
#     curl "${DATA_GOVERNANCE_API_URL}/<version>/entities?query=((type:SOURCE)AND(sourceType:Hive)AND(clusterName:<cluster>))" -u username:password -X GET
# Where:
# - <version> is the correct version, e.g. "v13". This can be identified using: curl "${DATA_GOVERNANCE_API_URL}/version" -X GET
# - <cluster> is the name of the cluster as shown in Navigator
# If multiple entities are listed then choose the correct source for the Hive service you intend to use
#export CLOUDERA_NAVIGATOR_HIVE_SOURCE_ID=
export DATA_GOVERNANCE_AUTO_TAGS='GOE,+RDBMS_NAME'
# Custom tags can be defined with a comma-separated string in DATA_GOVERNANCE_CUSTOM_TAGS
export DATA_GOVERNANCE_CUSTOM_TAGS=
export DATA_GOVERNANCE_AUTO_PROPERTIES='+GOE_OBJECT_TYPE,+SOURCE_RDBMS_TABLE,+TARGET_RDBMS_TABLE,+INITIAL_OPERATION_DATETIME,+LATEST_OPERATION_DATETIME'
# Custom properties can be included in GOE metadata via key/value pairs defined in DATA_GOVERNANCE_CUSTOM_PROPERTIES
#export DATA_GOVERNANCE_CUSTOM_PROPERTIES='{"key": "value", etc}'

# ===========================================================================================
# Advanced settings: you probably do not need to modify the lines below
# ===========================================================================================
# HDFS_CMD_HOST overrides HIVE_SERVER_HOST for HDFS operations only
export HDFS_CMD_HOST=

# Databases are named <schema><HDFS_DB_PATH_SUFFIX> and <schema>_load<HDFS_DB_PATH_SUFFIX>
#  HDFS_DB_PATH_SUFFIX defaults to .db, giving <schema>.db and <schema>_load.db
#  Uncomment the following line if the .db is causing problems
#export HDFS_DB_PATH_SUFFIX=

# Backend database and table name of the in-list-join table
#  can be created and populated with ./connect --create-sequence-table
#  defaults to default.goe_sequence
#export IN_LIST_JOIN_TABLE="default.goe_sequence"
#export IN_LIST_JOIN_TABLE_SIZE="10000"

# Backend distribution 'override' (supported values: CDH, HDP, EMR, MAPR)
export BACKEND_DISTRIBUTION=CDH

# Case conversion to be applied to any backend identifier names created by GOE (supported values: UPPER, LOWER and NO_MODIFY).
export BACKEND_IDENTIFIER_CASE=LOWER

# Key/value pairs, in JSON format, defining session query parameters for the orchestration backend query engine.
# These take effect for all queries issued to the query engine, e.g:
#     export OFFLOAD_BACKEND_SESSION_PARAMETERS="{\"request_pool\": \"goe.pool\"}"
#export OFFLOAD_BACKEND_SESSION_PARAMETERS=

# Orchestration query engine (supported values: IMPALA, HIVE)
export QUERY_ENGINE=IMPALA

# Comma-delimited list of HiveServer2 session parameters to set
# BATCH_SIZE=16384 is a strongly recommended performance setting
# eg. export HS2_SESSION_PARAMS="BATCH_SIZE=16384,MEM_LIMIT=2G"
export HS2_SESSION_PARAMS="BATCH_SIZE=16384"

# An alias provided by Hadoop "credential provided API" to be used for RDBMS authentication
# export OFFLOAD_TRANSPORT_PASSWORD_ALIAS=
# # The credential provider path to be used in conjunction with OFFLOAD_TRANSPORT_PASSWORD_ALIAS, e.g.:
# #     "jceks://hdfs/user/goe/dbname.dbuser.pwd.m.jceks"
# # Only required if the path is not configured in Hadoop configuration defaults
# export OFFLOAD_TRANSPORT_CREDENTIAL_PROVIDER_PATH=

# Sqoop settings:
# You should be using OraOOP optimizations for Sqoop (included in standard Apache Sqoop
# from v1.4.5), but if you're not, then you need to disable direct path mode:
export SQOOP_DISABLE_DIRECT=false
# Override flags for sqoop command, inserted right after "sqoop import", e.g.:
#     "-Dhadoop.security.credential.provider.path=jceks://hdfs/user/goe/dbname.dbuser.pwd.m.jceks"
# This setting is ignored for OFFLOAD_TRANSPORT_METHODs that do not utilise sqoop
export SQOOP_OVERRIDES="-Dsqoop.avro.logical_types.decimal.enable=false"
# Add sqoop command options at the end of the sqoop command
export SQOOP_ADDITIONAL_OPTIONS=
# HDFS path to Sqoop password file, readable by HADOOP_SSH_USER. If not specified, ORA_APP_PASS or OFFLOAD_TRANSPORT_PASSWORD_ALIAS will be used
export SQOOP_PASSWORD_FILE=
# Yarn queue name for GOE Sqoop jobs
export SQOOP_QUEUE_NAME=
